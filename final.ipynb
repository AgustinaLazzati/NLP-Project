{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "import requests\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tag import pos_tag\n",
    "import spacy\n",
    "import string\n",
    "from langdetect import detect\n",
    "from spellchecker import SpellChecker\n",
    "\n",
    "\n",
    "try:\n",
    "    nltk.data.find('tokenizers/punkt')\n",
    "except LookupError:\n",
    "    nltk.download('punkt')\n",
    "try:\n",
    "    nltk.data.find('taggers/averaged_perceptron_tagger')\n",
    "except LookupError:\n",
    "    nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "try:\n",
    "    nlp = spacy.load(\"es_core_news_sm\")\n",
    "except OSError:\n",
    "    print(\"Downloading Spanish model for spaCy...\")\n",
    "    spacy.cli.download(\"es_core_news_sm\")\n",
    "    nlp = spacy.load(\"es_core_news_sm\")\n",
    "\n",
    "train_url = 'https://raw.githubusercontent.com/AgustinaLazzati/NLP-Project/refs/heads/Test/negacio_train_v2024.json'\n",
    "test_url = 'https://raw.githubusercontent.com/AgustinaLazzati/NLP-Project/refs/heads/Test/negacio_test_v2024.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 1091836 words from the dictionary.\n"
     ]
    }
   ],
   "source": [
    "# Load spell checkers for Spanish and Catalan\n",
    "spell_es = SpellChecker(language='es')  # Spanish\n",
    "spell_ca = SpellChecker(language=None)  # Catalan (custom dictionary needed)\n",
    "\n",
    "# URL of the raw file in the GitHub repository\n",
    "file_url = \"https://raw.githubusercontent.com/AgustinaLazzati/NLP-Project/refs/heads/main/catala.txt\"\n",
    "\n",
    "# Fetch the file content from GitHub\n",
    "response = requests.get(file_url)\n",
    "\n",
    "if response.status_code == 200:\n",
    "    # If the request was successful, use the content\n",
    "    catalan_words = response.text.splitlines()\n",
    "    print(f\"Loaded {len(catalan_words)} words from the dictionary.\")\n",
    "else:\n",
    "    print(f\"Failed to retrieve the file. HTTP Status code: {response.status_code}\")\n",
    "\n",
    "# Now you can load the words into the spell checker\n",
    "spell_ca.word_frequency.load_words(catalan_words)\n",
    "\n",
    "def correct_misspellings(text, spell_checker):\n",
    "    def replace(match):\n",
    "        word = match.group(0)\n",
    "        corrected_word = spell_checker.correction(word)\n",
    "        return corrected_word if corrected_word else word  # Keep original if no suggestion\n",
    "\n",
    "    return re.sub(r'\\b\\w+\\b', replace, text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(url):\n",
    "    response = requests.get(url)\n",
    "    if response.status_code == 200: \n",
    "        try:\n",
    "            data = json.loads(response.text)\n",
    "            return data\n",
    "        except json.JSONDecodeError:\n",
    "            print(f\"Failed to parse JSON data from {url}\")\n",
    "            print(f\"First 500 characters of response: {response.text[:500]}\")\n",
    "            return None\n",
    "    else:\n",
    "        print(f\"Failed to load data: {response.status_code}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data loading and preprocessing functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected Language: Spanish\n",
      "El síndrome de don es una patología genética.\n"
     ]
    }
   ],
   "source": [
    "def load_data(url):\n",
    "    response = requests.get(url)\n",
    "    if response.status_code == 200:\n",
    "        try:\n",
    "            data = json.loads(response.text)\n",
    "            return data\n",
    "        except json.JSONDecodeError:\n",
    "            print(f\"Failed to parse JSON data from {url}\")\n",
    "            print(f\"First 500 characters of response: {response.text[:500]}\")\n",
    "            return None\n",
    "    else:\n",
    "        print(f\"Failed to load data: {response.status_code}\")\n",
    "        return None\n",
    "\n",
    "def get_text(sample):\n",
    "    if isinstance(sample, dict) and 'data' in sample and isinstance(sample['data'], dict):\n",
    "        return sample['data'].get('text', '')\n",
    "    return ''\n",
    "\n",
    "def preprocess_text(text):\n",
    "    if not isinstance(text, str) or not text:\n",
    "        return \"\"\n",
    "\n",
    "    # Handle redacted entities\n",
    "    text = re.sub(r'\\*+', '[REDACTED]', text)\n",
    "\n",
    "    # Replace common patterns for redacted information\n",
    "    text = re.sub(r'n[ºo]\\s*(historia|episodi|h\\.c\\.|h\\.c)[:\\s]*\\S+', '[HC_NUM]', text, flags=re.IGNORECASE)\n",
    "    text = re.sub(r'sexe:\\s*\\w+', '[GENDER]', text, flags=re.IGNORECASE)\n",
    "\n",
    "    # Normalize whitespace\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "\n",
    "    # Detect language\n",
    "    try:\n",
    "      language = detect(text)\n",
    "    except Exception as e:\n",
    "        print(f\"Language detection failed: {e}\")\n",
    "        language = 'unknown'  # If detection fails, assume unknown language\n",
    "\n",
    "    # Correct misspellings based on detected language\n",
    "    if language == 'ca':  # If Catalan is detected\n",
    "        print('Detected Language: Catalan')\n",
    "        text = correct_misspellings(text, spell_ca)\n",
    "    elif language == 'es':  # If Spanish is detected\n",
    "        print('Detected Language: Spanish')\n",
    "        text = correct_misspellings(text, spell_es)\n",
    "\n",
    "    return text.strip()\n",
    "\n",
    "\"\"\"\"\"\n",
    "def extract_features(text):\n",
    "    if not text:\n",
    "        return {\n",
    "            'tokens': [],\n",
    "            'lemmas': [],\n",
    "            'pos': [],\n",
    "            'is_punct': [],\n",
    "            'is_stop': [],\n",
    "            'spans': []\n",
    "        }\n",
    "\n",
    "    doc = nlp(text)\n",
    "\n",
    "    features = {\n",
    "        'tokens': [token.text for token in doc],\n",
    "        'lemmas': [token.lemma_ for token in doc],\n",
    "        'pos': [token.pos_ for token in doc],\n",
    "        'is_punct': [token.is_punct for token in doc],\n",
    "        'is_stop': [token.is_stop for token in doc],\n",
    "        'spans': [(token.idx, token.idx + len(token.text)) for token in doc]\n",
    "    }\n",
    "\n",
    "    return features\n",
    "\"\"\"\"\"\n",
    "\n",
    "\n",
    "# Example of how misspelling correction works\n",
    "text = \"El síndrome de down es una patología genetica.\"\n",
    "print(preprocess_text(text))  #makes incorrect corrections. add dictionary for medical words\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lists of negation and uncertainty cues in Spanish/Catalan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "NEGATION_PREFIX_CUES = [\n",
    "    \"no\", \"sin\", \"ausencia de\", \"ausencia\", \"negación\", \"negativo\", \"negativa\",\n",
    "    \"descarta\", \"descartado\", \"descartada\", \"descartar\", \"inexistente\", \"niega\",\n",
    "    \"rechaza\", \"libre de\", \"excluye\", \"excluido\", \"excluida\", \"no hay\", \"no se\",\n",
    "    \"no es\", \"no tiene\", \"nunca\", \"tampoco\", \"no presenta\", \"no muestra\",\n",
    "    \"no evidencia\", \"ni\", \"jamás\", \"sense\", \"absència\", \"cap\", \"exempt\", \"exempta\",\n",
    "    \"negatiu\", \"negativa\", \"nega\", \"descartat\", \"no hi ha\", \"res de\", \"no presenta\",\n",
    "    \"no existeix\", \"mai\", \"nul\", \"nul·la\", \"lliure de\", \"no consta\", \"exclou\",\n",
    "    \"gens de\", \"absents\", \"sense evidència de\", \"sense signes de\", \"no es detecta\",\n",
    "    \"no s'observa\", \"no és compatible amb\", \"no s'aprecia\", \"excluir\", \"excloure\",\n",
    "    \"denegar\", \"negar\", \"nada de\", \"ningún\", \"ninguna\", \"nunca\", \"ausentes\",\n",
    "    \"falta de\", \"carencia de\", \"déficit de\", \"eliminado\", \"eliminada\",\n",
    "    \"negado por\", \"descartándose\", \"normal\", \"normales\", \"dentro de límites normales\",\n",
    "    \"sense alteracions\", \"sense canvis\", \"normale\", \"normales\", \"normal para\"\n",
    "]\n",
    "\n",
    "NEGATION_POSTFIX_CUES = [\n",
    "    \"descartado\", \"descartada\", \"negado\", \"negada\", \"excluido\", \"excluida\",\n",
    "    \"ausente\", \"inexistente\", \"descartat\", \"negat\", \"exclòs\", \"exclosa\",\n",
    "    \"absent\", \"no detectado\", \"no detectada\", \"no apreciable\", \"no visualizado\",\n",
    "    \"no visualizada\", \"no present\", \"no visible\", \"no evidenciable\", \"no identificable\",\n",
    "    \"no identificado\", \"no identificada\", \"no hay\", \"no hi ha\", \"no existe\",\n",
    "    \"no existeix\", \"no observado\", \"no observada\", \"no s'observa\", \"no mostrado\",\n",
    "    \"no mostrada\", \"no demostrado\", \"no demostrada\", \"no apreciado\", \"no apreciada\",\n",
    "    \"dentro de límites normales\", \"sin alteraciones\", \"sense alteracions\"\n",
    "]\n",
    "\n",
    "UNCERTAINTY_CUES = [\n",
    "    \"posible\", \"probable\", \"quizás\", \"quizá\", \"tal vez\", \"posiblemente\",\n",
    "    \"probablemente\", \"parece\", \"sugiere\", \"sugestivo\", \"compatible con\",\n",
    "    \"podría\", \"puede\", \"puede ser\", \"pudiera\", \"sospecha\", \"sospechar\",\n",
    "    \"sospechado\", \"sospechada\", \"se sospecha\", \"duda\", \"en duda\", \"incierto\",\n",
    "    \"incierta\", \"inseguro\", \"insegura\", \"no claro\", \"no clara\", \"no descarta\",\n",
    "    \"potser\", \"possiblement\", \"probablement\", \"sembla\", \"suggereix\", \"compatible amb\",\n",
    "    \"podria\", \"pot\", \"pot ser\", \"sospita\", \"sospitar\", \"es sospita\", \"dubte\",\n",
    "    \"incert\", \"incerta\", \"no clar\", \"no clara\", \"dubtós\", \"dubtosa\", \"equívoc\",\n",
    "    \"equívoca\", \"a considerar\", \"a descartar\", \"no se puede excluir\", \"no es pot excloure\",\n",
    "    \"suggestivo/a de\", \"indeterminado\", \"indeterminada\", \"por determinar\", \"per determinar\",\n",
    "    \"por confirmar\", \"per confirmar\", \"a valorar\", \"en estudio\", \"en estudi\",\n",
    "    \"pendiente\", \"pendent\", \"a evaluar\", \"a evaluer\", \"interrogante\", \"interrogant\",\n",
    "    \"no concluyente\", \"no concluent\", \"eventual\", \"eventualment\", \"no definitivo\",\n",
    "    \"no definitiu\", \"impresiona\", \"impresión de\", \"impressió de\", \"presuntivo\",\n",
    "    \"presuntivo\", \"indicio\", \"indici\", \"sospecho\", \"potencial\", \"presumible\",\n",
    "    \"presumiblement\", \"aparente\", \"aparentment\", \"orientativo\", \"orientatiu\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset analysis and evaluation functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to get words and their positions\n",
    "def get_nextWord(text, End_word):\n",
    "    i, word, start = 0, \"\", True\n",
    "    while i < len(text):\n",
    "        if text[i] not in (' ', *End_word):\n",
    "            if start:\n",
    "                pos = i\n",
    "                start = False\n",
    "            word += text[i]\n",
    "        elif word:\n",
    "            yield (pos, word)\n",
    "            word, start = \"\", True\n",
    "        i += 1\n",
    "    yield (pos, word)\n",
    "\n",
    "# Function to get start and end labels\n",
    "def start_end_label(df, number_row):\n",
    "    data = df[\"predictions\"][number_row][0][\"result\"]\n",
    "    text = df[\"data\"][number_row][\"text\"]\n",
    "    return [(entry[\"value\"][\"start\"], entry[\"value\"][\"end\"], text[entry[\"value\"][\"start\"]:entry[\"value\"][\"end\"]],\n",
    "             len(text[entry[\"value\"][\"start\"]:entry[\"value\"][\"end\"]].rstrip().split()), entry[\"value\"][\"labels\"][0]) for entry in data]\n",
    "\n",
    "\n",
    "# Function to label text\n",
    "def label_text(df, number_row, End_word):\n",
    "    text = df[\"data\"][number_row][\"text\"]\n",
    "    labels = sorted(start_end_label(df, number_row), key=lambda x: x[0])\n",
    "    if not labels:\n",
    "        return []\n",
    "    pos, words_count, text_annotations = 0, 0, []\n",
    "    all_words = list(get_nextWord(text, End_word))\n",
    "    for index, word in all_words:\n",
    "        if pos < len(labels):\n",
    "            start, end, _, num_words, label = labels[pos]\n",
    "        text_annotations.append((word, label if start <= index <= end else 'other'))\n",
    "        words_count += 1 if start <= index <= end else 0\n",
    "        if words_count == num_words:\n",
    "            pos += 1\n",
    "            words_count = 0\n",
    "    return text_annotations\n",
    "\n",
    "\n",
    "# Function to obtain sentence delimiters\n",
    "def end_word_obtantion():\n",
    "    End_sentence = '!.?,;:'\n",
    "    End_word = ''.join(set(string.punctuation) - set(End_sentence))\n",
    "    return End_sentence, End_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to identify negation and uncertainty scope\n",
    "def identify_scope(text, End_sentence, End_word, negations_cues, uncertainty_cues):\n",
    "    sentences_of_text, sentence = [], []\n",
    "    negation_found, uncertainty_found = False, False\n",
    "    all_words = list(get_nextWord(text, End_word))\n",
    "    for _, token in all_words:\n",
    "        tag = (token, \"other\")\n",
    "        if token in negations_cues:\n",
    "            negation_found = True\n",
    "            tag = (token, \"NEG\")\n",
    "        elif token in uncertainty_cues:\n",
    "            uncertainty_found = True\n",
    "            tag = (token, \"UNC\")\n",
    "        sentence.append(tag)\n",
    "        if any(char in token for char in End_sentence):\n",
    "            sentence = [(t[0], \"NSCO\" if negation_found else \"USCO\") if t[1] == 'other' else t for t in sentence]\n",
    "            sentences_of_text.extend(sentence)\n",
    "            sentence, negation_found, uncertainty_found = [], False, False\n",
    "    return sentences_of_text\n",
    "\n",
    "\n",
    "# Function to pad sentences\n",
    "def padded_sentences(y_pred, y_true):\n",
    "    return [[(p if i < len(p) else (' ', 'other')) for i in range(len(t))] for p, t in zip(y_pred, y_true)], [[(t if i < len(t) else (' ', 'other')) for i in range(len(p))] for p, t in zip(y_pred, y_true)]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # Load data\n",
    "    train_data = load_data(train_url)\n",
    "    test_data = load_data(test_url)\n",
    "    \n",
    "    # Define negation and uncertainty cues\n",
    "    negations_cues = {\"no\", \"nunca\", \"jamás\", \"sin\"}\n",
    "    uncertainty_cues = {\"quizás\", \"tal vez\", \"posiblemente\"}\n",
    "    \n",
    "    # Process training data\n",
    "    if isinstance(train_data, dict) and \"data\" in train_data:\n",
    "        print(\"Processing training data...\")\n",
    "        for sample in train_data[\"data\"]:\n",
    "            text = preprocess_text(sample.get(\"text\", \"\"))\n",
    "            scope_result = identify_scope(text, negations_cues, uncertainty_cues)\n",
    "            print(scope_result)\n",
    "    \n",
    "    # Process test data\n",
    "    if isinstance(test_data, dict) and \"data\" in test_data:\n",
    "        print(\"Processing test data...\")\n",
    "        for sample in test_data[\"data\"]:\n",
    "            text = preprocess_text(sample.get(\"text\", \"\"))\n",
    "            scope_result = identify_scope(text, negations_cues, uncertainty_cues)\n",
    "            print(scope_result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
