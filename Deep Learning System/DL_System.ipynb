{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "a8770448",
      "metadata": {
        "id": "a8770448"
      },
      "source": [
        "### Fundamentals of Natural Language Processing\n",
        "# Negation and Uncertainty Detection using a Machine-Learning Based Approach\n",
        "\n",
        "*Authors:*\n",
        "\n",
        "> *Anna Blanco, Agustina Lazzati, Stanislav Bultaskii, Queralt Salvadó*\n",
        "\n",
        "*Aims:*\n",
        "> Rewrite for DL"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "OD99InlolDaK",
      "metadata": {
        "id": "OD99InlolDaK"
      },
      "outputs": [],
      "source": [
        "# Import necessary libraries and functions\n",
        "import json\n",
        "import spacy\n",
        "from collections import defaultdict\n",
        "import re\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import pickle"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"lstm_data.pkl\", \"rb\") as f:\n",
        "    data_dict = pickle.load(f)\n",
        "\n",
        "lstm_train_data_neg_cue = data_dict[\"lstm_train_data_neg_cue\"]\n",
        "lstm_train_data_neg_scope = data_dict[\"lstm_train_data_neg_scope\"]\n",
        "lstm_train_data_unc_cue = data_dict[\"lstm_train_data_unc_cue\"]\n",
        "lstm_train_data_unc_scope = data_dict[\"lstm_train_data_unc_scope\"]\n",
        "\n",
        "lstm_test_data_neg_cue = data_dict[\"lstm_test_data_neg_cue\"]\n",
        "lstm_test_data_neg_scope = data_dict[\"lstm_test_data_neg_scope\"]\n",
        "lstm_test_data_unc_cue = data_dict[\"lstm_test_data_unc_cue\"]\n",
        "lstm_test_data_unc_scope = data_dict[\"lstm_test_data_unc_scope\"]\n",
        "\n",
        "print(lstm_train_data_neg_cue[2])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YpRP1CIuFwHz",
        "outputId": "b9b782c8-6a09-4f71-c0a2-ff0fcff3d0c2"
      },
      "id": "YpRP1CIuFwHz",
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(['antecedents', 'alergia', 'a', 'penicilina', 'y', 'cloramfenicol', '.'], [0, 0, 0, 0, 0, 0, 0])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def merge_labels(cue_labels, scope_labels, cue_prefix=\"CUE\", scope_prefix=\"SCOPE\"):\n",
        "    merged = []\n",
        "    for cue, scope in zip(cue_labels, scope_labels):\n",
        "        if cue != 0:\n",
        "            merged.append(f\"{cue_prefix}_{str(cue)}\")\n",
        "        elif scope != 0:\n",
        "            merged.append(f\"{scope_prefix}_{str(scope)}\")\n",
        "        else:\n",
        "            merged.append(0)\n",
        "    return merged"
      ],
      "metadata": {
        "id": "pczB1GJeHk5G"
      },
      "id": "pczB1GJeHk5G",
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Merge negation data\n",
        "lstm_train_data_neg = [\n",
        "    (tokens, merge_labels(cue_labels, scope_labels, cue_prefix=\"NEG\", scope_prefix=\"NSCO\"))\n",
        "    for (tokens, cue_labels), (_, scope_labels) in zip(lstm_train_data_neg_cue, lstm_train_data_neg_scope)\n",
        "]\n",
        "\n",
        "lstm_test_data_neg = [\n",
        "    (tokens, merge_labels(cue_labels, scope_labels, cue_prefix=\"NEG\", scope_prefix=\"NSCO\"))\n",
        "    for (tokens, cue_labels), (_, scope_labels) in zip(lstm_test_data_neg_cue, lstm_test_data_neg_scope)\n",
        "]\n",
        "\n",
        "# Similarly for uncertainty\n",
        "lstm_train_data_neg = [\n",
        "    (tokens, merge_labels(cue_labels, scope_labels, cue_prefix=\"UNC\", scope_prefix=\"UNSCO\"))\n",
        "    for (tokens, cue_labels), (_, scope_labels) in zip(lstm_train_data_neg_cue, lstm_train_data_neg_scope)\n",
        "]\n",
        "\n",
        "lstm_test_data_neg = [\n",
        "    (tokens, merge_labels(cue_labels, scope_labels, cue_prefix=\"UNC\", scope_prefix=\"UNSCO\"))\n",
        "    for (tokens, cue_labels), (_, scope_labels) in zip(lstm_test_data_neg_cue, lstm_test_data_neg_scope)\n",
        "]\n",
        "\n",
        "print(lstm_train_data_neg[3])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4Cy16eUrHl38",
        "outputId": "2eb035ce-9abd-4859-f111-3f7bf298f4b4"
      },
      "id": "4Cy16eUrHl38",
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(['no', 'habitos', 'toxicos', '.'], ['UNC_1', 'UNSCO_1', 'UNSCO_1', 'UNSCO_1'])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install fasttext\n",
        "\n",
        "import fasttext\n",
        "\n",
        "# Download the English fastText model\n",
        "!wget https://dl.fbaipublicfiles.com/fasttext/vectors-crawl/cc.en.300.bin.gz\n",
        "# Unzip the downloaded file\n",
        "!gunzip cc.en.300.bin.gz"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LBh1-_eBGo0J",
        "outputId": "63214fa6-e11c-4f4c-91b4-d5e451b27ce6"
      },
      "id": "LBh1-_eBGo0J",
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting fasttext\n",
            "  Downloading fasttext-0.9.3.tar.gz (73 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/73.4 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m73.4/73.4 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting pybind11>=2.2 (from fasttext)\n",
            "  Using cached pybind11-2.13.6-py3-none-any.whl.metadata (9.5 kB)\n",
            "Requirement already satisfied: setuptools>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from fasttext) (75.2.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from fasttext) (2.0.2)\n",
            "Using cached pybind11-2.13.6-py3-none-any.whl (243 kB)\n",
            "Building wheels for collected packages: fasttext\n",
            "  Building wheel for fasttext (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fasttext: filename=fasttext-0.9.3-cp311-cp311-linux_x86_64.whl size=4313503 sha256=42008a00ca87415babb8e3c1534421c708597df1140618865080425f7fd33cc0\n",
            "  Stored in directory: /root/.cache/pip/wheels/65/4f/35/5057db0249224e9ab55a513fa6b79451473ceb7713017823c3\n",
            "Successfully built fasttext\n",
            "Installing collected packages: pybind11, fasttext\n",
            "Successfully installed fasttext-0.9.3 pybind11-2.13.6\n",
            "--2025-05-27 09:38:54--  https://dl.fbaipublicfiles.com/fasttext/vectors-crawl/cc.en.300.bin.gz\n",
            "Resolving dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)... 3.163.189.14, 3.163.189.96, 3.163.189.51, ...\n",
            "Connecting to dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)|3.163.189.14|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 4503593528 (4.2G) [application/octet-stream]\n",
            "Saving to: ‘cc.en.300.bin.gz’\n",
            "\n",
            "cc.en.300.bin.gz    100%[===================>]   4.19G   144MB/s    in 27s     \n",
            "\n",
            "2025-05-27 09:39:21 (161 MB/s) - ‘cc.en.300.bin.gz’ saved [4503593528/4503593528]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load pretrained FastText model (English, 300-dimensional vectors)\n",
        "fasttext_model = fasttext.load_model(\"cc.en.300.bin\")"
      ],
      "metadata": {
        "id": "tySN5ggvGv75"
      },
      "id": "tySN5ggvGv75",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "class LSTMTokenDataset(Dataset):\n",
        "    def __init__(self, data, ft_model, max_len=100):\n",
        "        self.data = data\n",
        "        self.ft = ft_model\n",
        "        self.max_len = max_len\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        tokens, labels = self.data[idx]\n",
        "\n",
        "        # Convert tokens to embeddings\n",
        "        embeddings = [torch.tensor(self.ft.get_word_vector(token.lower())) for token in tokens]\n",
        "\n",
        "        # Convert labels to tensor\n",
        "        label_tensor = torch.tensor(labels, dtype=torch.long)\n",
        "\n",
        "        return embeddings, label_tensor\n",
        "\n",
        "def collate_batch(batch):\n",
        "    embed_seqs, label_seqs = zip(*batch)\n",
        "\n",
        "    embed_seqs_padded = pad_sequence([torch.stack(seq) for seq in embed_seqs], batch_first=True)\n",
        "    label_seqs_padded = pad_sequence(label_seqs, batch_first=True, padding_value=-100)\n",
        "\n",
        "    return embed_seqs_padded, label_seqs_padded"
      ],
      "metadata": {
        "id": "ICuNAHquGyP0"
      },
      "id": "ICuNAHquGyP0",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset_neg = LSTMTokenDataset(lstm_train_data_neg_cue, ft_model)\n",
        "test_dataset_neg = LSTMTokenDataset(lstm_test_data_neg_cue, ft_model)\n",
        "\n",
        "train_loader_neg = DataLoader(train_dataset_neg, batch_size=32, shuffle=True, collate_fn=collate_batch)\n",
        "test_loader_neg = DataLoader(test_dataset_neg, batch_size=32, shuffle=False, collate_fn=collate_batch)\n"
      ],
      "metadata": {
        "id": "3LsHGQ0iG1qJ"
      },
      "id": "3LsHGQ0iG1qJ",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}