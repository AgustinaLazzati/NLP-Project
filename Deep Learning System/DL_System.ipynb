{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "a8770448",
      "metadata": {
        "id": "a8770448"
      },
      "source": [
        "### Fundamentals of Natural Language Processing\n",
        "# Negation and Uncertainty Detection using a Machine-Learning Based Approach\n",
        "\n",
        "*Authors:*\n",
        "\n",
        "> *Anna Blanco, Agustina Lazzati, Stanislav Bultaskii, Queralt Salvadó*\n",
        "\n",
        "*Aims:*\n",
        "> Rewrite for DL"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "OD99InlolDaK",
      "metadata": {
        "id": "OD99InlolDaK"
      },
      "outputs": [],
      "source": [
        "# Import necessary libraries and functions\n",
        "import json\n",
        "import spacy\n",
        "from collections import defaultdict\n",
        "import re\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import pickle"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"lstm_data.pkl\", \"rb\") as f:\n",
        "    data_dict = pickle.load(f)\n",
        "\n",
        "lstm_train_data_neg_cue = data_dict[\"lstm_train_data_neg_cue\"]\n",
        "lstm_train_data_neg_scope = data_dict[\"lstm_train_data_neg_scope\"]\n",
        "lstm_train_data_unc_cue = data_dict[\"lstm_train_data_unc_cue\"]\n",
        "lstm_train_data_unc_scope = data_dict[\"lstm_train_data_unc_scope\"]\n",
        "\n",
        "lstm_test_data_neg_cue = data_dict[\"lstm_test_data_neg_cue\"]\n",
        "lstm_test_data_neg_scope = data_dict[\"lstm_test_data_neg_scope\"]\n",
        "lstm_test_data_unc_cue = data_dict[\"lstm_test_data_unc_cue\"]\n",
        "lstm_test_data_unc_scope = data_dict[\"lstm_test_data_unc_scope\"]\n",
        "\n",
        "print(lstm_train_data_neg_cue[2])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YpRP1CIuFwHz",
        "outputId": "584a3759-d6ce-40e9-c373-1bfbe5c8f230"
      },
      "id": "YpRP1CIuFwHz",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(['antecedents', 'alergia', 'a', 'penicilina', 'y', 'cloramfenicol', '.'], [0, 0, 0, 0, 0, 0, 0])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def merge_labels(cue_labels, scope_labels, cue_prefix=\"CUE\", scope_prefix=\"SCOPE\"):\n",
        "    merged = []\n",
        "    for cue, scope in zip(cue_labels, scope_labels):\n",
        "        if cue != 0:\n",
        "            merged.append(f\"{cue_prefix}_{str(cue)}\")\n",
        "        elif scope != 0:\n",
        "            merged.append(f\"{scope_prefix}_{str(scope)}\")\n",
        "        else:\n",
        "            merged.append(\"0\")\n",
        "    return merged"
      ],
      "metadata": {
        "id": "pczB1GJeHk5G"
      },
      "id": "pczB1GJeHk5G",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Merge negation data\n",
        "lstm_train_data_neg = [\n",
        "    (tokens, merge_labels(cue_labels, scope_labels, cue_prefix=\"NEG\", scope_prefix=\"NSCO\"))\n",
        "    for (tokens, cue_labels), (_, scope_labels) in zip(lstm_train_data_neg_cue, lstm_train_data_neg_scope)\n",
        "]\n",
        "\n",
        "lstm_test_data_neg = [\n",
        "    (tokens, merge_labels(cue_labels, scope_labels, cue_prefix=\"NEG\", scope_prefix=\"NSCO\"))\n",
        "    for (tokens, cue_labels), (_, scope_labels) in zip(lstm_test_data_neg_cue, lstm_test_data_neg_scope)\n",
        "]\n",
        "\n",
        "# Similarly for uncertainty\n",
        "lstm_train_data_unc = [\n",
        "    (tokens, merge_labels(cue_labels, scope_labels, cue_prefix=\"UNC\", scope_prefix=\"UNSCO\"))\n",
        "    for (tokens, cue_labels), (_, scope_labels) in zip(lstm_train_data_unc_cue, lstm_train_data_unc_scope)\n",
        "]\n",
        "\n",
        "lstm_test_data_unc = [\n",
        "    (tokens, merge_labels(cue_labels, scope_labels, cue_prefix=\"UNC\", scope_prefix=\"UNSCO\"))\n",
        "    for (tokens, cue_labels), (_, scope_labels) in zip(lstm_test_data_unc_cue, lstm_test_data_unc_scope)\n",
        "]\n",
        "\n",
        "print(lstm_train_data_neg[3])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4Cy16eUrHl38",
        "outputId": "e684e645-267d-4d95-ddda-e8b3326d0a54"
      },
      "id": "4Cy16eUrHl38",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(['no', 'habitos', 'toxicos', '.'], ['NEG_1', 'NSCO_1', 'NSCO_1', 'NSCO_1'])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install fasttext\n",
        "\n",
        "import fasttext\n",
        "\n",
        "# Download the English fastText model\n",
        "!wget https://dl.fbaipublicfiles.com/fasttext/vectors-crawl/cc.en.300.bin.gz\n",
        "# Unzip the downloaded file\n",
        "!gunzip cc.en.300.bin.gz"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LBh1-_eBGo0J",
        "outputId": "dd36696c-67f3-4c02-fd43-11d0113cd8dc"
      },
      "id": "LBh1-_eBGo0J",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: fasttext in /usr/local/lib/python3.11/dist-packages (0.9.3)\n",
            "Requirement already satisfied: pybind11>=2.2 in /usr/local/lib/python3.11/dist-packages (from fasttext) (2.13.6)\n",
            "Requirement already satisfied: setuptools>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from fasttext) (75.2.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from fasttext) (2.0.2)\n",
            "--2025-05-27 10:09:37--  https://dl.fbaipublicfiles.com/fasttext/vectors-crawl/cc.en.300.bin.gz\n",
            "Resolving dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)... 3.163.189.51, 3.163.189.96, 3.163.189.108, ...\n",
            "Connecting to dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)|3.163.189.51|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 4503593528 (4.2G) [application/octet-stream]\n",
            "Saving to: ‘cc.en.300.bin.gz’\n",
            "\n",
            "cc.en.300.bin.gz    100%[===================>]   4.19G  7.14MB/s    in 58s     \n",
            "\n",
            "2025-05-27 10:10:34 (74.3 MB/s) - ‘cc.en.300.bin.gz’ saved [4503593528/4503593528]\n",
            "\n",
            "gzip: cc.en.300.bin already exists; do you wish to overwrite (y or n)? n\n",
            "\tnot overwritten\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load pretrained FastText model (English, 300-dimensional vectors)\n",
        "fasttext_model = fasttext.load_model(\"cc.en.300.bin\")"
      ],
      "metadata": {
        "id": "tySN5ggvGv75"
      },
      "id": "tySN5ggvGv75",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import defaultdict\n",
        "\n",
        "def build_vocab(sentences):\n",
        "    word2idx = defaultdict(lambda: 0)  # unknown token index = 0\n",
        "    idx = 1\n",
        "    for sent in sentences:\n",
        "        for word in sent:\n",
        "            if word not in word2idx:\n",
        "                word2idx[word] = idx\n",
        "                idx += 1\n",
        "    return dict(word2idx)\n",
        "\n",
        "def build_label_vocab(labels_list):\n",
        "    label_set = set()\n",
        "    for labels in labels_list:\n",
        "        label_set.update(labels)\n",
        "    label2idx = {label: i for i, label in enumerate(sorted(label_set))}\n",
        "    return label2idx\n",
        "\n",
        "# Train and Test data for negations\n",
        "all_train_sentences_neg = [x[0] for x in lstm_train_data_neg] # list of token lists\n",
        "all_train_labels_neg = [x[1] for x in lstm_train_data_neg] # list of label lists\n",
        "\n",
        "all_test_sentences_neg = [x[0] for x in lstm_test_data_neg]\n",
        "all_test_labels_neg = [x[1] for x in lstm_test_data_neg]\n",
        "\n",
        "# Train and Test data for uncertainties\n",
        "all_train_sentences_unc = [x[0] for x in lstm_train_data_unc]\n",
        "all_train_labels_unc = [x[1] for x in lstm_train_data_unc]\n",
        "\n",
        "all_test_sentences_unc = [x[0] for x in lstm_test_data_unc]\n",
        "all_test_labels_unc = [x[1] for x in lstm_test_data_unc]\n",
        "\n",
        "# Merge all sentences and labels into single lists\n",
        "all_sentences = (\n",
        "    all_train_sentences_neg + all_test_sentences_neg +\n",
        "    all_train_sentences_unc + all_test_sentences_unc\n",
        ")\n",
        "\n",
        "all_labels = (\n",
        "    all_train_labels_neg + all_test_labels_neg +\n",
        "    all_train_labels_unc + all_test_labels_unc\n",
        ")\n",
        "\n",
        "# Build vocabularies\n",
        "word2idx = build_vocab(all_sentences)\n",
        "label2idx = build_label_vocab(all_labels)\n",
        "\n",
        "print(f\"Vocabulary size (words): {len(word2idx)}\")\n",
        "print(f\"Number of unique labels: {len(label2idx)}\")\n",
        "\n",
        "# Optional: check example mappings\n",
        "print(f\"Example word2idx: {list(word2idx.items())[:10]}\")\n",
        "print(f\"Example label2idx: {list(label2idx.items())[:10]}\")"
      ],
      "metadata": {
        "id": "IplmnSStKkds",
        "outputId": "fe3c6983-e520-4b40-c971-5157647e149e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "IplmnSStKkds",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocabulary size (words): 23359\n",
            "Number of unique labels: 5\n",
            "Example word2idx: [(' ', 1), ('nº', 2), ('historia', 3), ('clinica', 4), (':', 5), ('*', 6), ('nºepisodi', 7), ('sexe', 8), ('home', 9), ('data', 10)]\n",
            "Example label2idx: [('0', 0), ('NEG_1', 1), ('NSCO_1', 2), ('UNC_1', 3), ('UNSCO_1', 4)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "vocab_size = len(word2idx) + 1  # +1 for padding idx=0\n",
        "embedding_matrix = np.zeros((vocab_size, 300))\n",
        "\n",
        "for word, idx in word2idx.items():\n",
        "    try:\n",
        "        embedding_vector = fasttext_model.get_word_vector(word)\n",
        "        embedding_matrix[idx] = embedding_vector\n",
        "    except KeyError:\n",
        "        embedding_matrix[idx] = np.random.normal(scale=0.6, size=(300,))"
      ],
      "metadata": {
        "id": "u4rGVxVcKuQg"
      },
      "id": "u4rGVxVcKuQg",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def encode_sentences(sentences, word2idx):\n",
        "    encoded = []\n",
        "    for sent in sentences:\n",
        "        encoded.append([word2idx.get(word, 0) for word in sent])\n",
        "    return encoded\n",
        "\n",
        "def encode_labels(labels, label2idx):\n",
        "    encoded = []\n",
        "    for lab_seq in labels:\n",
        "        encoded.append([label2idx[str(label)] for label in lab_seq])\n",
        "    return encoded\n",
        "\n",
        "# Negation data\n",
        "X_train_neg = encode_sentences(all_train_sentences_neg, word2idx)\n",
        "y_train_neg = encode_labels(all_train_labels_neg, label2idx)\n",
        "\n",
        "X_test_neg = encode_sentences(all_test_sentences_neg, word2idx)\n",
        "y_test_neg = encode_labels(all_test_labels_neg, label2idx)\n",
        "\n",
        "# Uncertainty data\n",
        "X_train_unc = encode_sentences(all_train_sentences_unc, word2idx)\n",
        "y_train_unc = encode_labels(all_train_labels_unc, label2idx)\n",
        "\n",
        "X_test_unc = encode_sentences(all_test_sentences_unc, word2idx)\n",
        "y_test_unc = encode_labels(all_test_labels_unc, label2idx)\n",
        "\n",
        "# Show an example\n",
        "print(X_train_neg[2], y_train_neg[2])"
      ],
      "metadata": {
        "id": "rhzvfE6DOkx5",
        "outputId": "f71387ac-0e79-42c1-d67f-bd164a8a3203",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "rhzvfE6DOkx5",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[45, 46, 47, 48, 49, 50, 44] [0, 0, 0, 0, 0, 0, 0]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.nn.utils.rnn import pad_sequence\n",
        "import torch\n",
        "\n",
        "def pad_sequences(sequences, pad_value=0):\n",
        "    # Convert lists of indices to torch tensors\n",
        "    tensor_seqs = [torch.tensor(seq) for seq in sequences]\n",
        "    # Pad sequences to the max length in the batch\n",
        "    padded_seqs = pad_sequence(tensor_seqs, batch_first=True, padding_value=pad_value)\n",
        "    return padded_seqs\n",
        "\n",
        "# Pad inputs and labels (negation)\n",
        "X_train_neg_padded = pad_sequences(X_train_neg, pad_value=0)\n",
        "y_train_neg_padded = pad_sequences(y_train_neg, pad_value=label2idx.get('0', 0))\n",
        "\n",
        "X_test_neg_padded = pad_sequences(X_test_neg, pad_value=0)\n",
        "y_test_neg_padded = pad_sequences(y_test_neg, pad_value=label2idx.get('0', 0))\n",
        "\n",
        "# Pad inputs and labels (uncertainty)\n",
        "X_train_unc_padded = pad_sequences(X_train_unc, pad_value=0)\n",
        "y_train_unc_padded = pad_sequences(y_train_unc, pad_value=label2idx.get('0', 0))\n",
        "\n",
        "X_test_unc_padded = pad_sequences(X_test_unc, pad_value=0)\n",
        "y_test_unc_padded = pad_sequences(y_test_unc, pad_value=label2idx.get('0', 0))"
      ],
      "metadata": {
        "id": "Z1qT8Ql5QIwh"
      },
      "id": "Z1qT8Ql5QIwh",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "class SequenceTaggingDataset(Dataset):\n",
        "    def __init__(self, X, y):\n",
        "        self.X = X\n",
        "        self.y = y\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.X)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.X[idx], self.y[idx]\n",
        "\n",
        "# Create dataset objects\n",
        "train_dataset_neg = SequenceTaggingDataset(X_train_neg_padded, y_train_neg_padded)\n",
        "test_dataset_neg = SequenceTaggingDataset(X_test_neg_padded, y_test_neg_padded)\n",
        "\n",
        "train_dataset_unc = SequenceTaggingDataset(X_train_unc_padded, y_train_unc_padded)\n",
        "test_dataset_unc = SequenceTaggingDataset(X_test_unc_padded, y_test_unc_padded)\n",
        "\n",
        "# Create dataloaders\n",
        "train_loader_neg = DataLoader(train_dataset_neg, batch_size=32, shuffle=True)\n",
        "test_loader_neg = DataLoader(test_dataset_neg, batch_size=32, shuffle=False)\n",
        "\n",
        "train_loader_unc = DataLoader(train_dataset_unc, batch_size=32, shuffle=True)\n",
        "test_loader_unc = DataLoader(test_dataset_unc, batch_size=32, shuffle=False)"
      ],
      "metadata": {
        "id": "yKd8eUM2P_jM"
      },
      "id": "yKd8eUM2P_jM",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "import torch\n",
        "\n",
        "class BiLSTM(nn.Module):\n",
        "    def __init__(self, embedding_matrix, hidden_dim_lstm, hidden_dim_gru, output_dim, pad_idx=0):\n",
        "        super(BiLSTM, self).__init__()\n",
        "        vocab_size, embedding_dim = embedding_matrix.shape\n",
        "\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=pad_idx)\n",
        "        self.embedding.weight.data.copy_(torch.from_numpy(embedding_matrix))\n",
        "        self.embedding.weight.requires_grad = False  # freeze embeddings if you want\n",
        "\n",
        "        self.lstm = nn.LSTM(embedding_dim, hidden_dim_lstm, batch_first=True, bidirectional=True)\n",
        "        self.gru = nn.GRU(hidden_dim_lstm * 2, hidden_dim_gru, batch_first=True, bidirectional=True)\n",
        "\n",
        "        self.fc = nn.Linear(hidden_dim_gru * 2, output_dim)  # times 2 for bidirectional GRU output\n",
        "\n",
        "    def forward(self, x):\n",
        "        embedded = self.embedding(x)  # (batch_size, seq_len, embedding_dim)\n",
        "\n",
        "        lstm_out, _ = self.lstm(embedded)  # (batch_size, seq_len, hidden_dim_lstm*2)\n",
        "        gru_out, _ = self.gru(lstm_out)    # (batch_size, seq_len, hidden_dim_gru*2)\n",
        "\n",
        "        logits = self.fc(gru_out)  # (batch_size, seq_len, output_dim)\n",
        "        return logits"
      ],
      "metadata": {
        "id": "C2Gg7YW1WPhr"
      },
      "id": "C2Gg7YW1WPhr",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.optim as optim\n",
        "import torch.nn as nn\n",
        "\n",
        "# Parameters (adjust as needed)\n",
        "hidden_dim_lstm = 128\n",
        "hidden_dim_gru = 64\n",
        "output_dim = len(label2idx)  # number of classes/tags\n",
        "pad_idx = 0\n",
        "\n",
        "# Instantiate model\n",
        "model = BiLSTM(embedding_matrix, hidden_dim_lstm, hidden_dim_gru, output_dim, pad_idx)\n",
        "\n",
        "# Move to GPU if available\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model = model.to(device)\n",
        "\n",
        "# Loss and optimizer\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=pad_idx)  # ignore padding labels in loss\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)"
      ],
      "metadata": {
        "id": "hgX6zmr0Wn2q"
      },
      "id": "hgX6zmr0Wn2q",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "\n",
        "def train_epoch(model, dataloader, optimizer, criterion, device):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "\n",
        "    for batch in dataloader:\n",
        "        inputs, labels = batch\n",
        "        inputs = inputs.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        outputs = model(inputs)  # outputs shape: (batch_size, seq_len, output_dim)\n",
        "\n",
        "        # Reshape for loss: combine batch and seq dims\n",
        "        outputs = outputs.view(-1, outputs.shape[-1])  # (batch_size * seq_len, output_dim)\n",
        "        labels = labels.view(-1)                       # (batch_size * seq_len)\n",
        "\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    return total_loss / len(dataloader)"
      ],
      "metadata": {
        "id": "9GC7Ce2fWpsm"
      },
      "id": "9GC7Ce2fWpsm",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_epochs = 5\n",
        "\n",
        "print(\"NEGATIONS\")\n",
        "for epoch in range(num_epochs):\n",
        "    train_loss = train_epoch(model, train_loader_neg, optimizer, criterion, device)\n",
        "    print(f\"Epoch {epoch+1}/{num_epochs} - Loss: {train_loss:.4f}\")"
      ],
      "metadata": {
        "id": "d6J5tTiIWtTe"
      },
      "id": "d6J5tTiIWtTe",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}