{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "### Data Preprocessing"
      ],
      "metadata": {
        "id": "HXFrIqCekooG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langdetect"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1q7071mNoRTu",
        "outputId": "cb3e5dab-8484-4735-c6e7-4304a0ef3325"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: langdetect in /usr/local/lib/python3.11/dist-packages (1.0.9)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.11/dist-packages (from langdetect) (1.17.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "id": "JwqozgX0khx3"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import re\n",
        "import requests\n",
        "import pandas as pd\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.tag import pos_tag\n",
        "import spacy\n",
        "import string\n",
        "from langdetect import detect\n",
        "\n",
        "try:\n",
        "    nltk.data.find('tokenizers/punkt')\n",
        "except LookupError:\n",
        "    nltk.download('punkt')\n",
        "try:\n",
        "    nltk.data.find('taggers/averaged_perceptron_tagger')\n",
        "except LookupError:\n",
        "    nltk.download('averaged_perceptron_tagger')\n",
        "\n",
        "try:\n",
        "    nlp = spacy.load(\"es_core_news_sm\")\n",
        "except OSError:\n",
        "    print(\"Downloading Spanish model for spaCy...\")\n",
        "    spacy.cli.download(\"es_core_news_sm\")\n",
        "    nlp = spacy.load(\"es_core_news_sm\")\n",
        "\n",
        "train_url = 'https://raw.githubusercontent.com/AgustinaLazzati/NLP-Project/refs/heads/Test/negacio_train_v2024.json'\n",
        "test_url = 'https://raw.githubusercontent.com/AgustinaLazzati/NLP-Project/refs/heads/Test/negacio_test_v2024.json'"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def load_data(url):\n",
        "    response = requests.get(url)\n",
        "    if response.status_code == 200:\n",
        "        try:\n",
        "            data = json.loads(response.text)\n",
        "            return data\n",
        "        except json.JSONDecodeError:\n",
        "            print(f\"Failed to parse JSON data from {url}\")\n",
        "            print(f\"First 500 characters of response: {response.text[:500]}\")\n",
        "            return None\n",
        "    else:\n",
        "        print(f\"Failed to load data: {response.status_code}\")\n",
        "        return None\n",
        "\n",
        "def get_text(sample):\n",
        "    if isinstance(sample, dict) and 'data' in sample and isinstance(sample['data'], dict):\n",
        "        return sample['data'].get('text', '')\n",
        "    return ''\n",
        "\n",
        "def preprocess_text(text):\n",
        "    if not isinstance(text, str) or not text:\n",
        "        return \"\"\n",
        "\n",
        "    # Handle redacted entities\n",
        "    text = re.sub(r'\\*+', '[REDACTED]', text)\n",
        "\n",
        "    # Replace common patterns for redacted information\n",
        "    text = re.sub(r'n[ºo]\\s*(historia|episodi|h\\.c\\.|h\\.c)[:\\s]*\\S+', '[HC_NUM]', text, flags=re.IGNORECASE)\n",
        "    text = re.sub(r'sexe:\\s*\\w+', '[GENDER]', text, flags=re.IGNORECASE)\n",
        "\n",
        "    # Normalize whitespace\n",
        "    text = re.sub(r'\\s+', ' ', text)\n",
        "\n",
        "    # Handle common misspellings (expand as needed)\n",
        "    misspellings = {\n",
        "        'sindrom': 'síndrome',\n",
        "        'patologí': 'patología',\n",
        "        'sintoma': 'síntoma',\n",
        "    }\n",
        "    for wrong, correct in misspellings.items():\n",
        "        text = re.sub(fr'\\b{wrong}\\b', correct, text, flags=re.IGNORECASE)\n",
        "\n",
        "    return text.strip()\n",
        "\n",
        "def extract_features(text):\n",
        "    if not text:\n",
        "        return {\n",
        "            'tokens': [],\n",
        "            'lemmas': [],\n",
        "            'pos': [],\n",
        "            'is_punct': [],\n",
        "            'is_stop': [],\n",
        "            'spans': []\n",
        "        }\n",
        "\n",
        "    doc = nlp(text)\n",
        "\n",
        "    features = {\n",
        "        'tokens': [token.text for token in doc],\n",
        "        'lemmas': [token.lemma_ for token in doc],\n",
        "        'pos': [token.pos_ for token in doc],\n",
        "        'is_punct': [token.is_punct for token in doc],\n",
        "        'is_stop': [token.is_stop for token in doc],\n",
        "        'spans': [(token.idx, token.idx + len(token.text)) for token in doc]\n",
        "    }\n",
        "\n",
        "    return features"
      ],
      "metadata": {
        "id": "WsnLLIcAk0Nj"
      },
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def split_into_sentences(text):\n",
        "    \"\"\"\n",
        "    Split text into sentences, one per line.\n",
        "    \"\"\"\n",
        "    if not isinstance(text, str) or not text.strip():\n",
        "        return []\n",
        "\n",
        "    # Preprocess the text first\n",
        "    processed_text = preprocess_text(text)\n",
        "\n",
        "    doc = nlp(processed_text)\n",
        "\n",
        "    # Extract sentences\n",
        "    sentences = [sent.text.strip() for sent in doc.sents if sent.text.strip()]\n",
        "\n",
        "    return sentences\n",
        "\n",
        "def process_data_to_sentences(data):\n",
        "    \"\"\"\n",
        "    Process the entire dataset to extract sentences, one per line.\n",
        "    \"\"\"\n",
        "    all_sentences = []\n",
        "\n",
        "    for sample in data:\n",
        "        text = get_text(sample)\n",
        "        if text:\n",
        "            sentences = split_into_sentences(text)\n",
        "            all_sentences.extend(sentences)\n",
        "\n",
        "    return all_sentences"
      ],
      "metadata": {
        "id": "Y7Vg7ObwpiMV"
      },
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "NEGATION_PREFIX_CUES = [\n",
        "    \"no\", \"sin\", \"ausencia de\", \"ausencia\", \"negación\", \"negativo\", \"negativa\",\n",
        "    \"descarta\", \"descartado\", \"descartada\", \"descartar\", \"inexistente\", \"niega\",\n",
        "    \"rechaza\", \"libre de\", \"excluye\", \"excluido\", \"excluida\", \"no hay\", \"no se\",\n",
        "    \"no es\", \"no tiene\", \"nunca\", \"tampoco\", \"no presenta\", \"no muestra\",\n",
        "    \"no evidencia\", \"ni\", \"jamás\", \"sense\", \"absència\", \"cap\", \"exempt\", \"exempta\",\n",
        "    \"negatiu\", \"negativa\", \"nega\", \"descartat\", \"no hi ha\", \"res de\", \"no presenta\",\n",
        "    \"no existeix\", \"mai\", \"nul\", \"nul·la\", \"lliure de\", \"no consta\", \"exclou\",\n",
        "    \"gens de\", \"absents\", \"sense evidència de\", \"sense signes de\", \"no es detecta\",\n",
        "    \"no s'observa\", \"no és compatible amb\", \"no s'aprecia\", \"excluir\", \"excloure\",\n",
        "    \"denegar\", \"negar\", \"nada de\", \"ningún\", \"ninguna\", \"nunca\", \"ausentes\",\n",
        "    \"falta de\", \"carencia de\", \"déficit de\", \"eliminado\", \"eliminada\",\n",
        "    \"negado por\", \"descartándose\", \"normal\", \"normales\", \"dentro de límites normales\",\n",
        "    \"sense alteracions\", \"sense canvis\", \"normale\", \"normales\", \"normal para\"\n",
        "]\n",
        "\n",
        "NEGATION_POSTFIX_CUES = [\n",
        "    \"descartado\", \"descartada\", \"negado\", \"negada\", \"excluido\", \"excluida\",\n",
        "    \"ausente\", \"inexistente\", \"descartat\", \"negat\", \"exclòs\", \"exclosa\",\n",
        "    \"absent\", \"no detectado\", \"no detectada\", \"no apreciable\", \"no visualizado\",\n",
        "    \"no visualizada\", \"no present\", \"no visible\", \"no evidenciable\", \"no identificable\",\n",
        "    \"no identificado\", \"no identificada\", \"no hay\", \"no hi ha\", \"no existe\",\n",
        "    \"no existeix\", \"no observado\", \"no observada\", \"no s'observa\", \"no mostrado\",\n",
        "    \"no mostrada\", \"no demostrado\", \"no demostrada\", \"no apreciado\", \"no apreciada\",\n",
        "    \"dentro de límites normales\", \"sin alteraciones\", \"sense alteracions\"\n",
        "]\n",
        "\n",
        "UNCERTAINTY_CUES = [\n",
        "    \"posible\", \"probable\", \"quizás\", \"quizá\", \"tal vez\", \"posiblemente\",\n",
        "    \"probablemente\", \"parece\", \"sugiere\", \"sugestivo\", \"compatible con\",\n",
        "    \"podría\", \"puede\", \"puede ser\", \"pudiera\", \"sospecha\", \"sospechar\",\n",
        "    \"sospechado\", \"sospechada\", \"se sospecha\", \"duda\", \"en duda\", \"incierto\",\n",
        "    \"incierta\", \"inseguro\", \"insegura\", \"no claro\", \"no clara\", \"no descarta\",\n",
        "    \"potser\", \"possiblement\", \"probablement\", \"sembla\", \"suggereix\", \"compatible amb\",\n",
        "    \"podria\", \"pot\", \"pot ser\", \"sospita\", \"sospitar\", \"es sospita\", \"dubte\",\n",
        "    \"incert\", \"incerta\", \"no clar\", \"no clara\", \"dubtós\", \"dubtosa\", \"equívoc\",\n",
        "    \"equívoca\", \"a considerar\", \"a descartar\", \"no se puede excluir\", \"no es pot excloure\",\n",
        "    \"suggestivo/a de\", \"indeterminado\", \"indeterminada\", \"por determinar\", \"per determinar\",\n",
        "    \"por confirmar\", \"per confirmar\", \"a valorar\", \"en estudio\", \"en estudi\",\n",
        "    \"pendiente\", \"pendent\", \"a evaluar\", \"a evaluer\", \"interrogante\", \"interrogant\",\n",
        "    \"no concluyente\", \"no concluent\", \"eventual\", \"eventualment\", \"no definitivo\",\n",
        "    \"no definitiu\", \"impresiona\", \"impresión de\", \"impressió de\", \"presuntivo\",\n",
        "    \"presuntivo\", \"indicio\", \"indici\", \"sospecho\", \"potencial\", \"presumible\",\n",
        "    \"presumiblement\", \"aparente\", \"aparentment\", \"orientativo\", \"orientatiu\"\n",
        "]"
      ],
      "metadata": {
        "id": "peoh3d2Wk3jT"
      },
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Once we've uploaded the necessary data we will implement a tokenizer in order to be able to convert the sentences in our data to tokens:"
      ],
      "metadata": {
        "id": "gF0Fmgm7n0Nh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize_spanish(text):\n",
        "    \"\"\"\n",
        "    Tokenizer for Spanish text using regex patterns.\n",
        "    Handles Spanish-specific contractions, abbreviations, and punctuation.\n",
        "    \"\"\"\n",
        "    # Common Spanish contractions and abbreviations\n",
        "    contractions = {\n",
        "        r\"\\bdel\\b\": \"de el\",\n",
        "        r\"\\bal\\b\": \"a el\",\n",
        "    }\n",
        "\n",
        "    # Apply contractions expansion\n",
        "    for pattern, replacement in contractions.items():\n",
        "        text = re.sub(pattern, replacement, text)\n",
        "\n",
        "    # Tokenization pattern:\n",
        "    pattern = r\"\"\"\n",
        "        \\w+'\\w+|\\w+-\\w+|            # Words with apostrophes or hyphens\n",
        "        [a-zA-ZáéíóúÁÉÍÓÚñÑüÜ]+|    # Spanish letters with diacritics\n",
        "        \\d+\\.?\\d*|                   # Numbers (including decimals)\n",
        "        [^\\w\\s]|                     # Any punctuation\n",
        "        \\S                           # Any non-whitespace (fallback)\n",
        "    \"\"\"\n",
        "\n",
        "    tokens = re.findall(pattern, text, re.VERBOSE)\n",
        "    return [token for token in tokens if token.strip()]\n",
        "\n",
        "def tokenize_catalan(text):\n",
        "    \"\"\"\n",
        "    Tokenizer for Catalan text using regex patterns.\n",
        "    Handles Catalan-specific contractions, abbreviations, and punctuation.\n",
        "    \"\"\"\n",
        "    # Common Catalan contractions and abbreviations\n",
        "    contractions = {\n",
        "        r\"\\bdel\\b\": \"de el\",        # Shared with Spanish\n",
        "        r\"\\bal\\b\": \"a el\",           # Shared with Spanish\n",
        "        r\"\\bpel\\b\": \"per el\",        # Catalan specific\n",
        "        r\"\\bvals\\b\": \"va els\",       # Catalan specific\n",
        "        r\"\\bca\\b\": \"casa\",           # Common abbreviation\n",
        "    }\n",
        "\n",
        "    # Apply contractions expansion\n",
        "    for pattern, replacement in contractions.items():\n",
        "        text = re.sub(pattern, replacement, text)\n",
        "\n",
        "    # Tokenization pattern:\n",
        "    pattern = r\"\"\"\n",
        "        \\w+['·]\\w+|\\w+-\\w+|         # Words with apostrophes, middle dots or hyphens\n",
        "        [a-zA-ZàèéíïòóúüÀÈÉÍÏÒÓÚÜÇç]+|  # Catalan letters with diacritics\n",
        "        \\d+\\.?\\d*|                  # Numbers (including decimals)\n",
        "        [^\\w\\s]|                    # Any punctuation\n",
        "        \\S                          # Any non-whitespace (fallback)\n",
        "    \"\"\"\n",
        "\n",
        "    tokens = re.findall(pattern, text, re.VERBOSE)\n",
        "    return [token for token in tokens if token.strip()]\n",
        "\n",
        "def tokenize_sentences(sentences):\n",
        "    tokenized_sentences = []\n",
        "\n",
        "    for sentence in sentences:\n",
        "        if not isinstance(sentence, str) or not sentence.strip():\n",
        "            # Skip empty or non-string entries\n",
        "            continue\n",
        "\n",
        "        if len(sentence.split()) < 3:\n",
        "            # Skip very short sentences that can't be reliably detected\n",
        "            tokens = tokenize_spanish(sentence)  # fallback to Spanish\n",
        "            tokenized_sentences.append(tokens)\n",
        "            continue\n",
        "\n",
        "        try:\n",
        "            # Detect language for each individual sentence\n",
        "            lang = detect(sentence)\n",
        "\n",
        "            # Tokenize based on detected language\n",
        "            if lang == 'es':\n",
        "                tokens = tokenize_spanish(sentence)\n",
        "            else:\n",
        "                tokens = tokenize_catalan(sentence)\n",
        "\n",
        "            tokenized_sentences.append(tokens)\n",
        "        except Exception as e:\n",
        "            # Fallback to Spanish tokenizer if detection fails\n",
        "            tokens = tokenize_spanish(sentence)\n",
        "            tokenized_sentences.append(tokens)\n",
        "\n",
        "    return tokenized_sentences"
      ],
      "metadata": {
        "id": "-eI20qGzn87n"
      },
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def main():\n",
        "    # Load data\n",
        "    train_data = load_data(train_url)\n",
        "    test_data = load_data(test_url)\n",
        "\n",
        "    # Preprocess data into sentences\n",
        "    train_sentences = process_data_to_sentences(train_data)\n",
        "\n",
        "    # Convert each sentence into tokens\n",
        "    tokenized_sentences = tokenize_sentences(train_sentences)\n",
        "\n",
        "    # Print the first few tokenized sentences for verification\n",
        "    for i, tokens in enumerate(tokenized_sentences[:5]):\n",
        "        print(f\"Sentence {i+1}: {tokens}\")\n",
        "\n",
        "main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8sXC3TRipyUc",
        "outputId": "c582f33f-1321-4936-86d0-7e2282a91d99"
      },
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sentence 1: ['[', 'HC', '_', 'NUM', ']', '[', 'REDACTED']\n",
            "Sentence 2: [']', '[', 'REDACTED', ']', '[', 'REDACTED', ']', '[', 'HC', '_', 'NUM', ']', '[', 'GENDER', ']', 'data', 'de', 'naixement', ':', '16.05', '.', '1936', 'edat', ':', '82', 'anys', 'procedencia', 'cex', 'mateix', 'hosp', 'servei', 'urologia', 'data', \"d'ingres\", '24.07', '.', '2018', 'data', \"d'alta\", '25.07', '.', '2018', '08', ':', '54', ':', '04', 'ates', 'per', '[', 'REDACTED', ']', ',', '[', 'REDACTED', ']', ';', '[', 'REDACTED', ']', ',', '[', 'REDACTED', ']', 'informe', \"d'alta\", \"d'hospitalitzacio\", 'motiu', \"d'ingres\", 'paciente', 'que', 'ingresa', 'de', 'forma', 'programada', 'para', 'realizacion', 'de', 'uretrotomia', 'interna', '.']\n",
            "Sentence 3: ['antecedents', 'alergia', 'a', 'penicilina', 'y', 'cloramfenicol', '.']\n",
            "Sentence 4: ['no', 'habitos', 'toxicos', '.']\n",
            "Sentence 5: ['antecedentes', 'medicos', ':', 'bloqueo', 'auriculoventricular', 'de', 'primer', 'grado', 'hipertension', 'arterial', '.']\n"
          ]
        }
      ]
    }
  ]
}