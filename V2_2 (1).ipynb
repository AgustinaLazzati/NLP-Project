{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install pyspellchecker\n",
        "!pip install langdetect"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IqWZrgSuJT7E",
        "outputId": "b51e2cfc-e0d2-4baa-c0ba-7c0cf73dbecd"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pyspellchecker in /usr/local/lib/python3.11/dist-packages (0.8.2)\n",
            "Collecting langdetect\n",
            "  Downloading langdetect-1.0.9.tar.gz (981 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m981.5/981.5 kB\u001b[0m \u001b[31m13.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.11/dist-packages (from langdetect) (1.17.0)\n",
            "Building wheels for collected packages: langdetect\n",
            "  Building wheel for langdetect (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for langdetect: filename=langdetect-1.0.9-py3-none-any.whl size=993222 sha256=372082b0731875d8715660183b33ad78f8eb2ac0305b6b43fb715e04f94f8d31\n",
            "  Stored in directory: /root/.cache/pip/wheels/0a/f2/b2/e5ca405801e05eb7c8ed5b3b4bcf1fcabcd6272c167640072e\n",
            "Successfully built langdetect\n",
            "Installing collected packages: langdetect\n",
            "Successfully installed langdetect-1.0.9\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "F-O6WiGZy41j"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import re\n",
        "import requests\n",
        "import pandas as pd\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.tag import pos_tag\n",
        "import spacy\n",
        "import string\n",
        "from langdetect import detect\n",
        "from spellchecker import SpellChecker\n",
        "\n",
        "\n",
        "try:\n",
        "    nltk.data.find('tokenizers/punkt')\n",
        "except LookupError:\n",
        "    nltk.download('punkt')\n",
        "try:\n",
        "    nltk.data.find('taggers/averaged_perceptron_tagger')\n",
        "except LookupError:\n",
        "    nltk.download('averaged_perceptron_tagger')\n",
        "\n",
        "try:\n",
        "    nlp = spacy.load(\"es_core_news_sm\")\n",
        "except OSError:\n",
        "    print(\"Downloading Spanish model for spaCy...\")\n",
        "    spacy.cli.download(\"es_core_news_sm\")\n",
        "    nlp = spacy.load(\"es_core_news_sm\")\n",
        "\n",
        "train_url = 'https://raw.githubusercontent.com/AgustinaLazzati/NLP-Project/refs/heads/Test/negacio_train_v2024.json'\n",
        "test_url = 'https://raw.githubusercontent.com/AgustinaLazzati/NLP-Project/refs/heads/Test/negacio_test_v2024.json'"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The following code defines a function used to correct misspellings on the given text. It uses the library SpellChecker to provide a"
      ],
      "metadata": {
        "id": "vc7AUJmcMQt5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load spell checkers for Spanish and Catalan\n",
        "spell_es = SpellChecker(language='es')  # Spanish\n",
        "spell_ca = SpellChecker(language=None)  # Catalan (custom dictionary needed)\n",
        "\n",
        "# URL of the raw file in the GitHub repository\n",
        "file_url = \"https://raw.githubusercontent.com/AgustinaLazzati/NLP-Project/refs/heads/main/catala.txt\"\n",
        "\n",
        "# Fetch the file content from GitHub\n",
        "response = requests.get(file_url)\n",
        "\n",
        "if response.status_code == 200:\n",
        "    # If the request was successful, use the content\n",
        "    catalan_words = response.text.splitlines()\n",
        "    print(f\"Loaded {len(catalan_words)} words from the dictionary.\")\n",
        "else:\n",
        "    print(f\"Failed to retrieve the file. HTTP Status code: {response.status_code}\")\n",
        "\n",
        "# Now you can load the words into the spell checker\n",
        "spell_ca.word_frequency.load_words(catalan_words)\n",
        "\n",
        "def correct_misspellings(text, spell_checker):\n",
        "    def replace(match):\n",
        "        word = match.group(0)\n",
        "        corrected_word = spell_checker.correction(word)\n",
        "        return corrected_word if corrected_word else word  # Keep original if no suggestion\n",
        "\n",
        "    return re.sub(r'\\b\\w+\\b', replace, text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GFb_A-uxJGFy",
        "outputId": "851e1549-0e71-4728-88d1-be79b408ab4c"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded 1091836 words from the dictionary.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n2nyGnnlzUnw"
      },
      "source": [
        "Data loading and preprocessing functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "b9gZ22zIy9cA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b3b34dba-319b-44e9-e8c2-c8a984a3745e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Detected Language: Spanish\n",
            "El síndrome de don es una patología genética.\n"
          ]
        }
      ],
      "source": [
        "def load_data(url):\n",
        "    response = requests.get(url)\n",
        "    if response.status_code == 200:\n",
        "        try:\n",
        "            data = json.loads(response.text)\n",
        "            return data\n",
        "        except json.JSONDecodeError:\n",
        "            print(f\"Failed to parse JSON data from {url}\")\n",
        "            print(f\"First 500 characters of response: {response.text[:500]}\")\n",
        "            return None\n",
        "    else:\n",
        "        print(f\"Failed to load data: {response.status_code}\")\n",
        "        return None\n",
        "\n",
        "def get_text(sample):\n",
        "    if isinstance(sample, dict) and 'data' in sample and isinstance(sample['data'], dict):\n",
        "        return sample['data'].get('text', '')\n",
        "    return ''\n",
        "\n",
        "def preprocess_text(text):\n",
        "    if not isinstance(text, str) or not text:\n",
        "        return \"\"\n",
        "\n",
        "    # Handle redacted entities\n",
        "    text = re.sub(r'\\*+', '[REDACTED]', text)\n",
        "\n",
        "    # Replace common patterns for redacted information\n",
        "    text = re.sub(r'n[ºo]\\s*(historia|episodi|h\\.c\\.|h\\.c)[:\\s]*\\S+', '[HC_NUM]', text, flags=re.IGNORECASE)\n",
        "    text = re.sub(r'sexe:\\s*\\w+', '[GENDER]', text, flags=re.IGNORECASE)\n",
        "\n",
        "    # Normalize whitespace\n",
        "    text = re.sub(r'\\s+', ' ', text)\n",
        "\n",
        "    # Detect language\n",
        "    try:\n",
        "      language = detect(text)\n",
        "    except Exception as e:\n",
        "        print(f\"Language detection failed: {e}\")\n",
        "        language = 'unknown'  # If detection fails, assume unknown language\n",
        "\n",
        "    # Correct misspellings based on detected language\n",
        "    if language == 'ca':  # If Catalan is detected\n",
        "        print('Detected Language: Catalan')\n",
        "        text = correct_misspellings(text, spell_ca)\n",
        "    elif language == 'es':  # If Spanish is detected\n",
        "        print('Detected Language: Spanish')\n",
        "        text = correct_misspellings(text, spell_es)\n",
        "\n",
        "    return text.strip()\n",
        "\n",
        "def extract_features(text):\n",
        "    if not text:\n",
        "        return {\n",
        "            'tokens': [],\n",
        "            'lemmas': [],\n",
        "            'pos': [],\n",
        "            'is_punct': [],\n",
        "            'is_stop': [],\n",
        "            'spans': []\n",
        "        }\n",
        "\n",
        "    doc = nlp(text)\n",
        "\n",
        "    features = {\n",
        "        'tokens': [token.text for token in doc],\n",
        "        'lemmas': [token.lemma_ for token in doc],\n",
        "        'pos': [token.pos_ for token in doc],\n",
        "        'is_punct': [token.is_punct for token in doc],\n",
        "        'is_stop': [token.is_stop for token in doc],\n",
        "        'spans': [(token.idx, token.idx + len(token.text)) for token in doc]\n",
        "    }\n",
        "\n",
        "    return features\n",
        "\n",
        "# Example of how misspelling correction works\n",
        "text = \"El síndrome de down es una patologí genetica.\"\n",
        "print(preprocess_text(text))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oChzS_JPzX0y"
      },
      "source": [
        "Lists of negation and uncertainty cues in Spanish/Catalan"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "8l4UzIppzR6P"
      },
      "outputs": [],
      "source": [
        "NEGATION_PREFIX_CUES = [\n",
        "    \"no\", \"sin\", \"ausencia de\", \"ausencia\", \"negación\", \"negativo\", \"negativa\",\n",
        "    \"descarta\", \"descartado\", \"descartada\", \"descartar\", \"inexistente\", \"niega\",\n",
        "    \"rechaza\", \"libre de\", \"excluye\", \"excluido\", \"excluida\", \"no hay\", \"no se\",\n",
        "    \"no es\", \"no tiene\", \"nunca\", \"tampoco\", \"no presenta\", \"no muestra\",\n",
        "    \"no evidencia\", \"ni\", \"jamás\", \"sense\", \"absència\", \"cap\", \"exempt\", \"exempta\",\n",
        "    \"negatiu\", \"negativa\", \"nega\", \"descartat\", \"no hi ha\", \"res de\", \"no presenta\",\n",
        "    \"no existeix\", \"mai\", \"nul\", \"nul·la\", \"lliure de\", \"no consta\", \"exclou\",\n",
        "    \"gens de\", \"absents\", \"sense evidència de\", \"sense signes de\", \"no es detecta\",\n",
        "    \"no s'observa\", \"no és compatible amb\", \"no s'aprecia\", \"excluir\", \"excloure\",\n",
        "    \"denegar\", \"negar\", \"nada de\", \"ningún\", \"ninguna\", \"nunca\", \"ausentes\",\n",
        "    \"falta de\", \"carencia de\", \"déficit de\", \"eliminado\", \"eliminada\",\n",
        "    \"negado por\", \"descartándose\", \"normal\", \"normales\", \"dentro de límites normales\",\n",
        "    \"sense alteracions\", \"sense canvis\", \"normale\", \"normales\", \"normal para\"\n",
        "]\n",
        "\n",
        "NEGATION_POSTFIX_CUES = [\n",
        "    \"descartado\", \"descartada\", \"negado\", \"negada\", \"excluido\", \"excluida\",\n",
        "    \"ausente\", \"inexistente\", \"descartat\", \"negat\", \"exclòs\", \"exclosa\",\n",
        "    \"absent\", \"no detectado\", \"no detectada\", \"no apreciable\", \"no visualizado\",\n",
        "    \"no visualizada\", \"no present\", \"no visible\", \"no evidenciable\", \"no identificable\",\n",
        "    \"no identificado\", \"no identificada\", \"no hay\", \"no hi ha\", \"no existe\",\n",
        "    \"no existeix\", \"no observado\", \"no observada\", \"no s'observa\", \"no mostrado\",\n",
        "    \"no mostrada\", \"no demostrado\", \"no demostrada\", \"no apreciado\", \"no apreciada\",\n",
        "    \"dentro de límites normales\", \"sin alteraciones\", \"sense alteracions\"\n",
        "]\n",
        "\n",
        "UNCERTAINTY_CUES = [\n",
        "    \"posible\", \"probable\", \"quizás\", \"quizá\", \"tal vez\", \"posiblemente\",\n",
        "    \"probablemente\", \"parece\", \"sugiere\", \"sugestivo\", \"compatible con\",\n",
        "    \"podría\", \"puede\", \"puede ser\", \"pudiera\", \"sospecha\", \"sospechar\",\n",
        "    \"sospechado\", \"sospechada\", \"se sospecha\", \"duda\", \"en duda\", \"incierto\",\n",
        "    \"incierta\", \"inseguro\", \"insegura\", \"no claro\", \"no clara\", \"no descarta\",\n",
        "    \"potser\", \"possiblement\", \"probablement\", \"sembla\", \"suggereix\", \"compatible amb\",\n",
        "    \"podria\", \"pot\", \"pot ser\", \"sospita\", \"sospitar\", \"es sospita\", \"dubte\",\n",
        "    \"incert\", \"incerta\", \"no clar\", \"no clara\", \"dubtós\", \"dubtosa\", \"equívoc\",\n",
        "    \"equívoca\", \"a considerar\", \"a descartar\", \"no se puede excluir\", \"no es pot excloure\",\n",
        "    \"suggestivo/a de\", \"indeterminado\", \"indeterminada\", \"por determinar\", \"per determinar\",\n",
        "    \"por confirmar\", \"per confirmar\", \"a valorar\", \"en estudio\", \"en estudi\",\n",
        "    \"pendiente\", \"pendent\", \"a evaluar\", \"a evaluer\", \"interrogante\", \"interrogant\",\n",
        "    \"no concluyente\", \"no concluent\", \"eventual\", \"eventualment\", \"no definitivo\",\n",
        "    \"no definitiu\", \"impresiona\", \"impresión de\", \"impressió de\", \"presuntivo\",\n",
        "    \"presuntivo\", \"indicio\", \"indici\", \"sospecho\", \"potencial\", \"presumible\",\n",
        "    \"presumiblement\", \"aparente\", \"aparentment\", \"orientativo\", \"orientatiu\"\n",
        "]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "86wXrJwrziXe"
      },
      "source": [
        "NegationDetector class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "JxRMFcdFzlGO"
      },
      "outputs": [],
      "source": [
        "class NegationDetector:\n",
        "    def __init__(self):\n",
        "        self.negation_prefix_patterns = self.compile_patterns(NEGATION_PREFIX_CUES)\n",
        "        self.negation_postfix_patterns = self.compile_patterns(NEGATION_POSTFIX_CUES)\n",
        "        self.uncertainty_patterns = self.compile_patterns(UNCERTAINTY_CUES)\n",
        "        self.scope_window = 7  # Increased from 5 to capture larger scopes\n",
        "\n",
        "    def compile_patterns(self, cue_list):\n",
        "        patterns = []\n",
        "        for cue in cue_list:\n",
        "            if ' ' in cue:\n",
        "                pattern = r'\\b' + re.escape(cue) + r'\\b'\n",
        "            else:\n",
        "                pattern = r'\\b' + re.escape(cue) + r'\\b'\n",
        "            patterns.append(re.compile(pattern, re.IGNORECASE))\n",
        "        return patterns\n",
        "\n",
        "    def find_matches(self, text, patterns):\n",
        "        matches = []\n",
        "        for pattern in patterns:\n",
        "            for match in pattern.finditer(text):\n",
        "                matches.append({\n",
        "                    'start': match.start(),\n",
        "                    'end': match.end(),\n",
        "                    'cue': text[match.start():match.end()]\n",
        "                })\n",
        "        return matches\n",
        "\n",
        "    def detect_negation_cues(self, text):\n",
        "        prefix_matches = self.find_matches(text, self.negation_prefix_patterns)\n",
        "        postfix_matches = self.find_matches(text, self.negation_postfix_patterns)\n",
        "\n",
        "        cues = []\n",
        "        for match in prefix_matches:\n",
        "            match['type'] = 'negation_prefix'\n",
        "            cues.append(match)\n",
        "        for match in postfix_matches:\n",
        "            match['type'] = 'negation_postfix'\n",
        "            cues.append(match)\n",
        "        return cues\n",
        "\n",
        "    def detect_uncertainty_cues(self, text):\n",
        "        \"\"\"Detect uncertainty cues in text\"\"\"\n",
        "        matches = self.find_matches(text, self.uncertainty_patterns)\n",
        "        cues = []\n",
        "        for match in matches:\n",
        "            match['type'] = 'uncertainty'\n",
        "            cues.append(match)\n",
        "        return cues\n",
        "\n",
        "    def detect_scope(self, text, cues, features):\n",
        "        if not text or not cues or not features['tokens']:\n",
        "            return []\n",
        "\n",
        "        scopes = []\n",
        "        doc = nlp(text)\n",
        "        tokens = [token for token in doc]\n",
        "\n",
        "        for cue in cues:\n",
        "            cue_start = cue['start']\n",
        "            cue_token_idx = None\n",
        "            for i, token in enumerate(tokens):\n",
        "                if token.idx <= cue_start < token.idx + len(token.text):\n",
        "                    cue_token_idx = i\n",
        "                    break\n",
        "\n",
        "            if cue_token_idx is None:\n",
        "                continue\n",
        "\n",
        "            if cue['type'] == 'negation_prefix':\n",
        "                scope_start = cue['end']\n",
        "                scope_end_idx = min(cue_token_idx + self.scope_window + 1, len(tokens))\n",
        "                if scope_end_idx < len(tokens):\n",
        "                    scope_end = tokens[scope_end_idx].idx\n",
        "                else:\n",
        "                    scope_end = len(text)\n",
        "\n",
        "            elif cue['type'] == 'negation_postfix':\n",
        "                scope_end = cue['start']\n",
        "                scope_start_idx = max(0, cue_token_idx - self.scope_window)\n",
        "                scope_start = tokens[scope_start_idx].idx\n",
        "\n",
        "            elif cue['type'] == 'uncertainty':\n",
        "                scope_start = cue['end']\n",
        "                scope_end_idx = min(cue_token_idx + self.scope_window + 1, len(tokens))\n",
        "                if scope_end_idx < len(tokens):\n",
        "                    scope_end = tokens[scope_end_idx].idx\n",
        "                else:\n",
        "                    scope_end = len(text)\n",
        "\n",
        "            for i, token in enumerate(tokens[cue_token_idx:min(cue_token_idx + self.scope_window + 1, len(tokens))]):\n",
        "                if token.is_punct and token.text in ['.', ';', ':', '!', '?']:\n",
        "                    if cue['type'] in ['negation_prefix', 'uncertainty']:\n",
        "                        scope_end = token.idx\n",
        "                        break\n",
        "\n",
        "            if cue['type'] == 'negation_postfix':\n",
        "                for i in range(cue_token_idx - 1, max(0, cue_token_idx - self.scope_window - 1), -1):\n",
        "                    if tokens[i].is_punct and tokens[i].text in ['.', ';', ':', '!', '?']:\n",
        "                        scope_start = tokens[i+1].idx if i+1 < len(tokens) else tokens[i].idx + len(tokens[i].text)\n",
        "                        break\n",
        "\n",
        "            scope_text = text[scope_start:scope_end].strip()\n",
        "            if scope_text:\n",
        "                scopes.append({\n",
        "                    'cue': cue,\n",
        "                    'scope_start': scope_start,\n",
        "                    'scope_end': scope_end,\n",
        "                    'scope_text': scope_text\n",
        "                })\n",
        "\n",
        "        return scopes\n",
        "\n",
        "    def process_text(self, text):\n",
        "        cleaned_text = preprocess_text(text)\n",
        "        if not cleaned_text:\n",
        "            return {\n",
        "                'text': \"\",\n",
        "                'negation_cues': [],\n",
        "                'uncertainty_cues': [],\n",
        "                'scopes': []\n",
        "            }\n",
        "\n",
        "        features = extract_features(cleaned_text)\n",
        "        negation_cues = self.detect_negation_cues(cleaned_text)\n",
        "        uncertainty_cues = self.detect_uncertainty_cues(cleaned_text)\n",
        "        all_cues = negation_cues + uncertainty_cues\n",
        "        scopes = self.detect_scope(cleaned_text, all_cues, features)\n",
        "\n",
        "        return {\n",
        "            'text': cleaned_text,\n",
        "            'negation_cues': negation_cues,\n",
        "            'uncertainty_cues': uncertainty_cues,\n",
        "            'scopes': scopes\n",
        "        }"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QbIbXt6Dzso3"
      },
      "source": [
        "Dataset analysis and evaluation functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "qSXRo42Vzmf-"
      },
      "outputs": [],
      "source": [
        "def analyze_dataset_statistics(data):\n",
        "    if not data:\n",
        "        return \"No data available for analysis.\"\n",
        "\n",
        "    result = {\n",
        "        \"num_samples\": len(data),\n",
        "        \"has_text\": 0,\n",
        "        \"sample_keys\": set(),\n",
        "        \"avg_text_length\": 0,\n",
        "        \"text_lengths\": [],\n",
        "        \"languages\": {\"spanish\": 0, \"catalan\": 0, \"unknown\": 0}\n",
        "    }\n",
        "\n",
        "    total_length = 0\n",
        "\n",
        "    if data and isinstance(data[0], dict):\n",
        "        result[\"sample_keys\"] = set(data[0].keys())\n",
        "\n",
        "    for sample in data:\n",
        "        if isinstance(sample, dict):\n",
        "            text = get_text(sample)\n",
        "            if text:\n",
        "                result[\"has_text\"] += 1\n",
        "                text_length = len(text)\n",
        "                result[\"text_lengths\"].append(text_length)\n",
        "                total_length += text_length\n",
        "\n",
        "                text_lower = text.lower()\n",
        "                if any(word in text_lower for word in [\"paciente\", \"presenta\", \"día\", \"hospital\", \"médico\"]):\n",
        "                    result[\"languages\"][\"spanish\"] += 1\n",
        "                elif any(word in text_lower for word in [\"pacient\", \"presenta\", \"dia\", \"hospital\", \"metge\"]):\n",
        "                    result[\"languages\"][\"catalan\"] += 1\n",
        "                else:\n",
        "                    result[\"languages\"][\"unknown\"] += 1\n",
        "\n",
        "    if result[\"has_text\"] > 0:\n",
        "        result[\"avg_text_length\"] = total_length / result[\"has_text\"]\n",
        "\n",
        "    return result\n",
        "\n",
        "def evaluate_model(predictions, gold_standard):\n",
        "    true_pos_cues = 0\n",
        "    false_pos_cues = 0\n",
        "    false_neg_cues = 0\n",
        "    true_pos_scopes = 0\n",
        "    false_pos_scopes = 0\n",
        "    false_neg_scopes = 0\n",
        "\n",
        "    if true_pos_cues + false_pos_cues > 0:\n",
        "        precision_cues = true_pos_cues / (true_pos_cues + false_pos_cues)\n",
        "    else:\n",
        "        precision_cues = 0\n",
        "\n",
        "    if true_pos_cues + false_neg_cues > 0:\n",
        "        recall_cues = true_pos_cues / (true_pos_cues + false_neg_cues)\n",
        "    else:\n",
        "        recall_cues = 0\n",
        "\n",
        "    if precision_cues + recall_cues > 0:\n",
        "        f1_cues = 2 * (precision_cues * recall_cues) / (precision_cues + recall_cues)\n",
        "    else:\n",
        "        f1_cues = 0\n",
        "\n",
        "    if true_pos_scopes + false_pos_scopes > 0:\n",
        "        precision_scopes = true_pos_scopes / (true_pos_scopes + false_pos_scopes)\n",
        "    else:\n",
        "        precision_scopes = 0\n",
        "\n",
        "    if true_pos_scopes + false_neg_scopes > 0:\n",
        "        recall_scopes = true_pos_scopes / (true_pos_scopes + false_neg_scopes)\n",
        "    else:\n",
        "        recall_scopes = 0\n",
        "\n",
        "    if precision_scopes + recall_scopes > 0:\n",
        "        f1_scopes = 2 * (precision_scopes * recall_scopes) / (precision_scopes + recall_scopes)\n",
        "    else:\n",
        "        f1_scopes = 0\n",
        "\n",
        "    return {\n",
        "        'cues': {\n",
        "            'precision': precision_cues,\n",
        "            'recall': recall_cues,\n",
        "            'f1': f1_cues\n",
        "        },\n",
        "        'scopes': {\n",
        "            'precision': precision_scopes,\n",
        "            'recall': recall_scopes,\n",
        "            'f1': f1_scopes\n",
        "        }\n",
        "    }"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JpRIg5RAzvf8"
      },
      "source": [
        "Main execution and testing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fYlH_TilzxbF",
        "outputId": "3c33cb10-817e-460e-ae9a-64a08c53890f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading training data...\n",
            "Loading test data...\n",
            "\n",
            "Analyzing training dataset...\n",
            "Training dataset statistics: {'num_samples': 254, 'has_text': 254, 'sample_keys': {'data', 'predictions', 'annotations'}, 'avg_text_length': 5042.673228346457, 'text_lengths': [3866, 1401, 4271, 10349, 2958, 5888, 5886, 3647, 2822, 2099, 9312, 4467, 5663, 2387, 8348, 2725, 5443, 2535, 4847, 2713, 4257, 2956, 4626, 1749, 6102, 9401, 2965, 10551, 8686, 6128, 3220, 9682, 11741, 7088, 7698, 5720, 4668, 5344, 5962, 10647, 1597, 3762, 1797, 4248, 3771, 1679, 1856, 4554, 2415, 1491, 3764, 3950, 13715, 3882, 4009, 4539, 2553, 4259, 11501, 3343, 1989, 2071, 10209, 2258, 13481, 4241, 13566, 11883, 5190, 2257, 1802, 5597, 6898, 3444, 7826, 10773, 6031, 2408, 2588, 3444, 13037, 2575, 6411, 22202, 4075, 3655, 6762, 3621, 5090, 6705, 3309, 4714, 3637, 6754, 11352, 6741, 10928, 3329, 4192, 6227, 6505, 6060, 2229, 6667, 7181, 1664, 4754, 5162, 1454, 10466, 6166, 3922, 3213, 4887, 9417, 3757, 10755, 5804, 5797, 8930, 3444, 1157, 8524, 8505, 4112, 4036, 3631, 8128, 1824, 4636, 2979, 3444, 1798, 1636, 2269, 6840, 2979, 2684, 6403, 1777, 3987, 3839, 4634, 4619, 2061, 4440, 4746, 3897, 2701, 7523, 5782, 5427, 4622, 1900, 3796, 1958, 2799, 3655, 2073, 5572, 2717, 4034, 3667, 6221, 5266, 2202, 2942, 1902, 1949, 3788, 4127, 15466, 4539, 2078, 3394, 5720, 3229, 4947, 4771, 3313, 4774, 2474, 1536, 5768, 4577, 3603, 2690, 7912, 4150, 1933, 4104, 5199, 12526, 3958, 5768, 6824, 3638, 3845, 10032, 4609, 6581, 2313, 2345, 2095, 7930, 3226, 2828, 3898, 2862, 4313, 5394, 3208, 2633, 8282, 5201, 5847, 7570, 3078, 11216, 11981, 7156, 2979, 2979, 2450, 2807, 5968, 11352, 2522, 4316, 3172, 4853, 2014, 6271, 11365, 6545, 3444, 7418, 3479, 5811, 5324, 2129, 7838, 2664, 2689, 4712, 4581, 6167, 1482, 3811, 2158, 7192, 3263, 4048, 4485], 'languages': {'spanish': 254, 'catalan': 0, 'unknown': 0}}\n",
            "\n",
            "Analyzing test dataset...\n",
            "Test dataset statistics: {'num_samples': 64, 'has_text': 64, 'sample_keys': {'data', 'predictions', 'annotations'}, 'avg_text_length': 5144.359375, 'text_lengths': [4351, 7641, 11739, 11145, 13382, 697, 3073, 907, 3531, 2147, 4611, 3555, 4263, 7985, 4085, 6813, 3732, 3121, 2052, 13483, 3444, 11545, 9361, 2260, 2879, 2260, 12886, 2452, 2746, 8876, 5556, 3581, 3854, 3444, 3444, 2710, 4263, 1391, 4482, 1671, 2457, 6204, 6751, 2313, 6103, 3242, 3656, 12744, 3626, 2731, 5316, 2151, 4454, 12491, 4442, 3128, 3590, 4738, 2677, 5501, 3896, 7431, 4638, 9541], 'languages': {'spanish': 64, 'catalan': 0, 'unknown': 0}}\n",
            "\n",
            "Processing samples from training data...\n",
            "\n",
            "--- Sample 1 ---\n",
            "Original text (first 100 chars):  nº historia clinica: ** *** *** nºepisodi: ******** sexe: home data de naixement: 16.05.1936 edat: ...\n",
            "Cleaned text (first 100 chars): [HC_NUM] [REDACTED] [REDACTED] [REDACTED] [HC_NUM] [GENDER] data de naixement: 16.05.1936 edat: 82 a...\n",
            "Found 11 negation cues and 1 uncertainty cues\n",
            "Sample negation cues:\n",
            "- no (443:445)\n",
            "- no (1983:1985)\n",
            "- sin (992:995)\n",
            "Sample uncertainty cues:\n",
            "- puede (3210:3215)\n",
            "Sample detected scopes:\n",
            "- Cue: no, Scope: 'habitos toxicos'\n",
            "- Cue: no, Scope: 'permite el paso de una guia'\n",
            "- Cue: sin, Scope: 'exito'\n",
            "\n",
            "--- Sample 2 ---\n",
            "Original text (first 100 chars):  nº historia clinica: ** *** *** nºepisodi: ******** sexe: dona data de naixement: 04.08.2000 edat: ...\n",
            "Cleaned text (first 100 chars): [HC_NUM] [REDACTED] [REDACTED] [REDACTED] [HC_NUM] [GENDER] data de naixement: 04.08.2000 edat: 19 a...\n",
            "Found 5 negation cues and 0 uncertainty cues\n",
            "Sample negation cues:\n",
            "- no (340:342)\n",
            "- no (380:382)\n",
            "- no (464:466)\n",
            "Sample detected scopes:\n",
            "- Cue: no, Scope: 'al·lergies medicamentoses conegudes'\n",
            "- Cue: no, Scope: 'intervencions quirurgiques ni altres antecedents patologics'\n",
            "- Cue: no, Scope: 'medicacio habitual'\n",
            "\n",
            "--- Sample 3 ---\n",
            "Original text (first 100 chars):  nº historia clinica: ** *** *** nºepisodi: ******** sexe: home data de naixement: 16.03.1952 edat: ...\n",
            "Cleaned text (first 100 chars): [HC_NUM] [REDACTED] [REDACTED] [REDACTED] [HC_NUM] [GENDER] data de naixement: 16.03.1952 edat: 66 a...\n",
            "Found 10 negation cues and 0 uncertainty cues\n",
            "Sample negation cues:\n",
            "- no (1174:1176)\n",
            "- no (2400:2402)\n",
            "- sin (383:386)\n",
            "Sample detected scopes:\n",
            "- Cue: no, Scope: 'doloroso a la palpacion sin signos de'\n",
            "- Cue: no, Scope: 'bien tolerada, requiriendo nueva colocacion de'\n",
            "- Cue: sin, Scope: 'alergias medicamentosas conocidas - diabetes mellitus tipo'\n",
            "\n",
            "--- Sample 4 ---\n",
            "Original text (first 100 chars):  nº historia clinica: ** *** *** nºepisodi: ******** sexe: home data de naixement: 14.03.1953 edat: ...\n",
            "Cleaned text (first 100 chars): [HC_NUM] [REDACTED] [REDACTED] [REDACTED] [HC_NUM] [GENDER] data de naixement: 14.03.1953 edat: 66 a...\n",
            "Found 38 negation cues and 10 uncertainty cues\n",
            "Sample negation cues:\n",
            "- no (399:401)\n",
            "- no (754:756)\n",
            "- no (1428:1430)\n",
            "Sample uncertainty cues:\n",
            "- posible (7781:7788)\n",
            "- probable (8263:8271)\n",
            "- podria (3781:3787)\n",
            "Sample detected scopes:\n",
            "- Cue: no, Scope: 'alergias medicamentosas'\n",
            "- Cue: no, Scope: 'signisficativa clinicamente'\n",
            "- Cue: no, Scope: 'eran presentes en estudio de junio 2018'\n",
            "\n",
            "--- Sample 5 ---\n",
            "Original text (first 100 chars):  nº historia clinica: ** *** *** nºepisodi: ******** sexe: dona data de naixement: 26.12.1924 edat: ...\n",
            "Cleaned text (first 100 chars): [HC_NUM] [REDACTED] [REDACTED] [REDACTED] [HC_NUM] [GENDER] data de naixement: 26.12.1924 edat: 93 a...\n",
            "Found 6 negation cues and 0 uncertainty cues\n",
            "Sample negation cues:\n",
            "- no (424:426)\n",
            "- no (723:725)\n",
            "- no (816:818)\n",
            "Sample detected scopes:\n",
            "- Cue: no, Scope: 'amc medicacio habitual sedotime 45 mg 1/24'\n",
            "- Cue: no, Scope: 'demencia'\n",
            "- Cue: no, Scope: 'hematomas impotencia funcional exploracio complementaria rx'\n",
            "\n",
            "--- Overall Statistics ---\n",
            "Total cues found in samples: 81\n",
            "Total scopes found in samples: 77\n",
            "Cue type distribution:\n",
            "- negation_prefix: 68 (84.0%)\n",
            "- uncertainty: 11 (13.6%)\n",
            "- negation_postfix: 2 (2.5%)\n",
            "\n",
            "Processing all test data...\n",
            "Processed 64 test samples.\n",
            "Found 1499 negation cues and 137 uncertainty cues.\n",
            "Detected 1477 scopes in total.\n"
          ]
        }
      ],
      "source": [
        "def main():\n",
        "    print(\"Loading training data...\")\n",
        "    train_data = load_data(train_url)\n",
        "    print(\"Loading test data...\")\n",
        "    test_data = load_data(test_url)\n",
        "\n",
        "    if not train_data or not test_data:\n",
        "        print(\"Failed to load data. Exiting.\")\n",
        "        return\n",
        "\n",
        "    print(\"\\nAnalyzing training dataset...\")\n",
        "    train_stats = analyze_dataset_statistics(train_data)\n",
        "    print(f\"Training dataset statistics: {train_stats}\")\n",
        "\n",
        "    print(\"\\nAnalyzing test dataset...\")\n",
        "    test_stats = analyze_dataset_statistics(test_data)\n",
        "    print(f\"Test dataset statistics: {test_stats}\")\n",
        "\n",
        "    detector = NegationDetector()\n",
        "\n",
        "    print(\"\\nProcessing samples from training data...\")\n",
        "\n",
        "    all_cues = []\n",
        "    all_scopes = []\n",
        "\n",
        "    for i, sample in enumerate(train_data[:5]):\n",
        "        text = get_text(sample)\n",
        "        if text:\n",
        "            print(f\"\\n--- Sample {i+1} ---\")\n",
        "            print(f\"Original text (first 100 chars): {text[:100]}...\")\n",
        "\n",
        "            result = detector.process_text(text)\n",
        "            print(f\"Cleaned text (first 100 chars): {result['text'][:100]}...\")\n",
        "\n",
        "            n_cues = len(result['negation_cues']) + len(result['uncertainty_cues'])\n",
        "            all_cues.extend(result['negation_cues'])\n",
        "            all_cues.extend(result['uncertainty_cues'])\n",
        "            all_scopes.extend(result['scopes'])\n",
        "\n",
        "            print(f\"Found {len(result['negation_cues'])} negation cues and {len(result['uncertainty_cues'])} uncertainty cues\")\n",
        "\n",
        "            if result['negation_cues']:\n",
        "                print(\"Sample negation cues:\")\n",
        "                for cue in result['negation_cues'][:3]:\n",
        "                    print(f\"- {cue['cue']} ({cue['start']}:{cue['end']})\")\n",
        "\n",
        "            if result['uncertainty_cues']:\n",
        "                print(\"Sample uncertainty cues:\")\n",
        "                for cue in result['uncertainty_cues'][:3]:\n",
        "                    print(f\"- {cue['cue']} ({cue['start']}:{cue['end']})\")\n",
        "\n",
        "            if result['scopes']:\n",
        "                print(\"Sample detected scopes:\")\n",
        "                for scope in result['scopes'][:3]:\n",
        "                    print(f\"- Cue: {scope['cue']['cue']}, Scope: '{scope['scope_text']}'\")\n",
        "\n",
        "    print(\"\\n--- Overall Statistics ---\")\n",
        "    print(f\"Total cues found in samples: {len(all_cues)}\")\n",
        "    print(f\"Total scopes found in samples: {len(all_scopes)}\")\n",
        "\n",
        "    if all_cues:\n",
        "        cue_types = {}\n",
        "        for cue in all_cues:\n",
        "            cue_type = cue['type']\n",
        "            if cue_type in cue_types:\n",
        "                cue_types[cue_type] += 1\n",
        "            else:\n",
        "                cue_types[cue_type] = 1\n",
        "\n",
        "        print(\"Cue type distribution:\")\n",
        "        for cue_type, count in cue_types.items():\n",
        "            print(f\"- {cue_type}: {count} ({count/len(all_cues)*100:.1f}%)\")\n",
        "\n",
        "    print(\"\\nProcessing all test data...\")\n",
        "    results = []\n",
        "    total_negation_cues = 0\n",
        "    total_uncertainty_cues = 0\n",
        "    total_scopes = 0\n",
        "\n",
        "    for sample in test_data:\n",
        "        text = get_text(sample)\n",
        "        if text:\n",
        "            result = detector.process_text(text)\n",
        "            results.append(result)\n",
        "            total_negation_cues += len(result['negation_cues'])\n",
        "            total_uncertainty_cues += len(result['uncertainty_cues'])\n",
        "            total_scopes += len(result['scopes'])\n",
        "\n",
        "    print(f\"Processed {len(results)} test samples.\")\n",
        "    print(f\"Found {total_negation_cues} negation cues and {total_uncertainty_cues} uncertainty cues.\")\n",
        "    print(f\"Detected {total_scopes} scopes in total.\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}