{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fundamentals of Natural Language Processing\n",
    "# Negation and Uncertainty Detection using a Machine-Learning Based Approach\n",
    "\n",
    "*Authors:*\n",
    "\n",
    "> *Anna Blanco, Agustina Lazzati, Stanislav Bultaskii, Queralt SalvadÃ³*\n",
    "\n",
    "*Aims:*\n",
    "> Our goal is to train various Machine Learning based models for each of the two sub-tasks (detection of negation and uncertainty signals, and detection of the negation and uncertainty scopes)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries and functions\n",
    "import json\n",
    "import pandas as pd\n",
    "import spacy\n",
    "from utils import preprocess_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First of all we will load all the data as a dictionary, remove unnecessary information such as '*', normalize whitespaces, and convert it into a proper format, so that we can then work with it. To do this, we will separate the texts from the prediction information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "def load_data(path):\n",
    "    \"\"\"Load JSON data and return raw dict\"\"\"\n",
    "    with open(path) as f:\n",
    "        return json.load(f)\n",
    "\n",
    "def preprocess_data(data):\n",
    "    \"\"\"Extract texts and predictions from loaded data\"\"\"\n",
    "    texts = []\n",
    "    predictions = []\n",
    "\n",
    "    for i in range(len(data)):\n",
    "        sentence = preprocess_text(data['data'][i]['text'], keep_case=False)\n",
    "        texts.append(sentence)\n",
    "        predictions.append(data['predictions'][i])\n",
    "\n",
    "    return texts, predictions\n",
    "\n",
    "# Load train and test data\n",
    "train_data = load_data('negacio_train_v2024.json')\n",
    "test_data = load_data('negacio_test_v2024.json')\n",
    "\n",
    "# Convert data into a DataFrame for simplicity\n",
    "train_df = pd.DataFrame(train_data)\n",
    "test_df = pd.DataFrame(test_data)\n",
    "\n",
    "# Preprocess it and obtain both the texts and the predictions\n",
    "train_texts, train_preds = preprocess_data(train_df)\n",
    "test_texts, test_preds = preprocess_data(test_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we will define a function to extract only the annotations from the predictions, those annotations will be used later to align them with the token indices:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_annotations_grouped(preds):\n",
    "    \"\"\"Extracts annotations from the predictions format, grouped per text\"\"\"\n",
    "    all_annotations = []\n",
    "\n",
    "    for pred in preds:\n",
    "        text_annotations = []\n",
    "        for result_entry in pred:\n",
    "            for result in result_entry['result']:\n",
    "                value = result['value']\n",
    "                text_annotations.append({\n",
    "                    'start': value['start'],\n",
    "                    'end': value['end'],\n",
    "                    'labels': value['labels']\n",
    "                })\n",
    "        all_annotations.append(text_annotations)\n",
    "\n",
    "    return all_annotations\n",
    "\n",
    "train_annotations = extract_annotations_grouped(train_preds)\n",
    "test_annotations = extract_annotations_grouped(test_preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use `spacy` for tokenization and sentence splitting:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('xx_ent_wiki_sm') # supports both spanish and catalan\n",
    "\n",
    "def tokenize_sentences(data, nlp):\n",
    "    \"\"\"Splits texts into tokens using spaCy\"\"\"\n",
    "    # List to store resulting tokens\n",
    "    tokenized_data = []\n",
    "\n",
    "    for text in data:\n",
    "        # tokenize sentence using spacy model\n",
    "        doc = nlp(text)\n",
    "        # split data into tokens, avoiding white space tokens\n",
    "        tokens = [token.text for token in doc if not token.is_space]\n",
    "        # add resulting list to the tokens list\n",
    "        tokenized_data.append(tokens)\n",
    "    \n",
    "    return tokenized_data\n",
    "\n",
    "train_tokens = tokenize_sentences(train_texts, nlp)\n",
    "test_tokens = tokenize_sentences(test_texts, nlp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Map character-level annotations(e.g. `'start': 347, 'end': 350`) to token indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def char_to_token_indices(texts, annotations_list, nlp):\n",
    "    \"\"\"Maps character-level annotation spans to token indices using spaCy.\"\"\"\n",
    "    all_token_annotations = []\n",
    "\n",
    "    for text, annotations in zip(texts, annotations_list):\n",
    "        doc = nlp(text)\n",
    "        token_annotations = []\n",
    "        for ann in annotations:\n",
    "            start, end = ann['start'], ann['end']\n",
    "            span = doc.char_span(start, end, alignment_mode='contract')\n",
    "            if span:\n",
    "                token_annotations.append((span.start, span.end, ann['labels'][0]))\n",
    "            else:\n",
    "                pass\n",
    "        all_token_annotations.append(token_annotations)\n",
    "\n",
    "    return all_token_annotations\n",
    "\n",
    "train_token_annotations = char_to_token_indices(train_texts, train_annotations, nlp)\n",
    "test_token_annotations = char_to_token_indices(test_texts, test_annotations, nlp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Now, the indexes we're seeing correspond to the tokens rather than to the characters, the label tells us whether it is a negation or uncertainty cue or scope.*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
