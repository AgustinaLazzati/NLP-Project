{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fundamentals of Natural Language Processing\n",
    "# Negation and Uncertainty Detection using a Machine-Learning Based Approach\n",
    "\n",
    "*Authors:*\n",
    "\n",
    "> *Anna Blanco, Agustina Lazzati, Stanislav Bultaskii, Queralt Salvadó*\n",
    "\n",
    "*Aims:*\n",
    "> Our goal is to train various Machine Learning based models for each of the two sub-tasks (detection of negation and uncertainty signals, and detection of the negation and uncertainty scopes). In order to do so, we followed the implementation method described by *Enger, Velldal, and Øvrelid (2017)*, which employs a maximum-margin approach for negation detection. However, for our particular application, we also included uncertainty cues and scope detection."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*References:* \n",
    "<br>\n",
    "> Enger, M., Velldal, E., & Øvrelid, L. (2017). *An open-source tool for negation detection: A maximum-margin approach*. Proceedings of the Workshop on Computational Semantics Beyond Events and Roles (SemBEaR), 64–69."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can erase this if you want but the thing is that we need to use the environment that queralt did. You need to write some commands to have the nlp_project (Python) as we have specific libraries. \n",
    "\n",
    "I did that and in the preprocessing it worked but here in order to work I had to run this command above, if it is not needed just avoid them!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['es_core_news_sm']\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "# Check installed models\n",
    "print(spacy.util.get_installed_models())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!python -m spacy download es_core_news_sm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   sentence_id  token_id      word     lemma    pos prefix suffix  is_punct  \\\n",
      "0            0         0                      SPACE                       0   \n",
      "1            1         0        nº        nº   NOUN     nº     nº         0   \n",
      "2            1         1  historia  historia   NOUN    his    ria         0   \n",
      "3            1         2   clinica   clinico    ADJ    cli    ica         0   \n",
      "4            1         3         :         :  PUNCT      :      :         1   \n",
      "\n",
      "   is_redacted    dep head_pos  in_single_word_cues  in_affixal_cues  \\\n",
      "0            0    dep    SPACE                    0                0   \n",
      "1            0    det     NOUN                    0                0   \n",
      "2            0   ROOT     NOUN                    0                0   \n",
      "3            0   amod     NOUN                    0                0   \n",
      "4            0  punct     NOUN                    0                0   \n",
      "\n",
      "   ends_with_ment  neg_cue_label  \n",
      "0               0              0  \n",
      "1               0              0  \n",
      "2               0              0  \n",
      "3               0              0  \n",
      "4               0              0  \n",
      "---------------------------------------\n",
      "\n",
      "\n",
      "   sentence_id  token_id      word     lemma    pos prefix suffix  is_punct  \\\n",
      "0            0         0                      SPACE                       0   \n",
      "1            1         0        nº        nº   NOUN     nº     nº         0   \n",
      "2            1         1  historia  historia   NOUN    his    ria         0   \n",
      "3            1         2   clinica   clinico    ADJ    cli    ica         0   \n",
      "4            1         3         :         :  PUNCT      :      :         1   \n",
      "\n",
      "   is_redacted    dep head_pos  in_single_word_cues  in_affixal_cues  \\\n",
      "0            0    dep    SPACE                    0                0   \n",
      "1            0    det     NOUN                    0                0   \n",
      "2            0  nsubj     VERB                    0                0   \n",
      "3            0   amod     NOUN                    0                0   \n",
      "4            0  punct     NOUN                    0                0   \n",
      "\n",
      "   ends_with_ment  neg_cue_label  \n",
      "0               0              0  \n",
      "1               0              0  \n",
      "2               0              0  \n",
      "3               0              0  \n",
      "4               0              0  \n",
      "---------------------------------------\n",
      "\n",
      "\n",
      "   sentence_id  token_id      word     lemma    pos prefix suffix  is_punct  \\\n",
      "0            0         0                      SPACE                       0   \n",
      "1            1         0        nº        nº   NOUN     nº     nº         0   \n",
      "2            1         1  historia  historia   NOUN    his    ria         0   \n",
      "3            1         2   clinica   clinico    ADJ    cli    ica         0   \n",
      "4            1         3         :         :  PUNCT      :      :         1   \n",
      "\n",
      "   is_redacted    dep head_pos  in_single_word_cues  in_affixal_cues  \\\n",
      "0            0    dep    SPACE                    0                0   \n",
      "1            0    det     NOUN                    0                0   \n",
      "2            0   ROOT     NOUN                    0                0   \n",
      "3            0   amod     NOUN                    0                0   \n",
      "4            0  punct     NOUN                    0                0   \n",
      "\n",
      "   ends_with_ment  unc_cue_label  \n",
      "0               0              0  \n",
      "1               0              0  \n",
      "2               0              0  \n",
      "3               0              0  \n",
      "4               0              0  \n",
      "---------------------------------------\n",
      "\n",
      "\n",
      "   sentence_id  token_id      word     lemma    pos prefix suffix  is_punct  \\\n",
      "0            0         0                      SPACE                       0   \n",
      "1            1         0        nº        nº   NOUN     nº     nº         0   \n",
      "2            1         1  historia  historia   NOUN    his    ria         0   \n",
      "3            1         2   clinica   clinico    ADJ    cli    ica         0   \n",
      "4            1         3         :         :  PUNCT      :      :         1   \n",
      "\n",
      "   is_redacted    dep head_pos  in_single_word_cues  in_affixal_cues  \\\n",
      "0            0    dep    SPACE                    0                0   \n",
      "1            0    det     NOUN                    0                0   \n",
      "2            0   ROOT     NOUN                    0                0   \n",
      "3            0   amod     NOUN                    0                0   \n",
      "4            0  punct     NOUN                    0                0   \n",
      "\n",
      "   ends_with_ment  unc_cue_label  \n",
      "0               0              0  \n",
      "1               0              0  \n",
      "2               0              0  \n",
      "3               0              0  \n",
      "4               0              0  \n",
      "---------------------------------------\n",
      "\n",
      "\n",
      "   sentence_id  token_id      word     lemma    pos prefix suffix  is_punct  \\\n",
      "0            0         0                      SPACE                       0   \n",
      "1            1         0        nº        nº   NOUN     nº     nº         0   \n",
      "2            1         1  historia  historia   NOUN    his    ria         0   \n",
      "3            1         2   clinica   clinico    ADJ    cli    ica         0   \n",
      "4            1         3         :         :  PUNCT      :      :         1   \n",
      "\n",
      "   is_redacted    dep head_pos  in_single_word_cues  in_affixal_cues  \\\n",
      "0            0    dep    SPACE                    0                0   \n",
      "1            0    det     NOUN                    0                0   \n",
      "2            0   ROOT     NOUN                    0                0   \n",
      "3            0   amod     NOUN                    0                0   \n",
      "4            0  punct     NOUN                    0                0   \n",
      "\n",
      "   ends_with_ment  neg_scope_label  \n",
      "0               0                0  \n",
      "1               0                0  \n",
      "2               0                0  \n",
      "3               0                0  \n",
      "4               0                0  \n",
      "---------------------------------------\n",
      "\n",
      "\n",
      "       sentence_id  token_id       word      lemma    pos prefix suffix  \\\n",
      "0                0         0                        SPACE                 \n",
      "1                1         0         nº         nº   NOUN     nº     nº   \n",
      "2                1         1   historia   historia   NOUN    his    ria   \n",
      "3                1         2    clinica    clinico    ADJ    cli    ica   \n",
      "4                1         3          :          :  PUNCT      :      :   \n",
      "...            ...       ...        ...        ...    ...    ...    ...   \n",
      "65526         3459        23          *          *    NUM      *      *   \n",
      "65527         3459        24          )          )  PUNCT      )      )   \n",
      "65528         3459        25  age-v-mir  age-v-mir   VERB    age    mir   \n",
      "65529         3459        26        4/4        4/4    NUM    4/4    4/4   \n",
      "65530         3459        27       lopd       lopd    ADJ    lop    opd   \n",
      "\n",
      "       is_punct  is_redacted    dep head_pos  in_single_word_cues  \\\n",
      "0             0            0    dep    SPACE                    0   \n",
      "1             0            0    det     NOUN                    0   \n",
      "2             0            0  nsubj     VERB                    0   \n",
      "3             0            0   amod     NOUN                    0   \n",
      "4             1            0  punct     NOUN                    0   \n",
      "...         ...          ...    ...      ...                  ...   \n",
      "65526         1            1  punct     NOUN                    0   \n",
      "65527         1            0  punct     NOUN                    0   \n",
      "65528         0            0  appos     NOUN                    0   \n",
      "65529         0            0  appos     NOUN                    0   \n",
      "65530         0            0   amod     NOUN                    0   \n",
      "\n",
      "       in_affixal_cues  ends_with_ment  neg_scope_label  \n",
      "0                    0               0                0  \n",
      "1                    0               0                0  \n",
      "2                    0               0                0  \n",
      "3                    0               0                0  \n",
      "4                    0               0                0  \n",
      "...                ...             ...              ...  \n",
      "65526                0               0                0  \n",
      "65527                0               0                0  \n",
      "65528                1               0                0  \n",
      "65529                0               0                0  \n",
      "65530                0               0                0  \n",
      "\n",
      "[65531 rows x 15 columns]\n",
      "---------------------------------------\n",
      "\n",
      "\n",
      "   sentence_id  token_id      word     lemma    pos prefix suffix  is_punct  \\\n",
      "0            0         0                      SPACE                       0   \n",
      "1            1         0        nº        nº   NOUN     nº     nº         0   \n",
      "2            1         1  historia  historia   NOUN    his    ria         0   \n",
      "3            1         2   clinica   clinico    ADJ    cli    ica         0   \n",
      "4            1         3         :         :  PUNCT      :      :         1   \n",
      "\n",
      "   is_redacted    dep head_pos  in_single_word_cues  in_affixal_cues  \\\n",
      "0            0    dep    SPACE                    0                0   \n",
      "1            0    det     NOUN                    0                0   \n",
      "2            0   ROOT     NOUN                    0                0   \n",
      "3            0   amod     NOUN                    0                0   \n",
      "4            0  punct     NOUN                    0                0   \n",
      "\n",
      "   ends_with_ment  unc_scope_label  \n",
      "0               0                0  \n",
      "1               0                0  \n",
      "2               0                0  \n",
      "3               0                0  \n",
      "4               0                0  \n",
      "---------------------------------------\n",
      "\n",
      "\n",
      "   sentence_id  token_id      word     lemma    pos prefix suffix  is_punct  \\\n",
      "0            0         0                      SPACE                       0   \n",
      "1            1         0        nº        nº   NOUN     nº     nº         0   \n",
      "2            1         1  historia  historia   NOUN    his    ria         0   \n",
      "3            1         2   clinica   clinico    ADJ    cli    ica         0   \n",
      "4            1         3         :         :  PUNCT      :      :         1   \n",
      "\n",
      "   is_redacted    dep head_pos  in_single_word_cues  in_affixal_cues  \\\n",
      "0            0    dep    SPACE                    0                0   \n",
      "1            0    det     NOUN                    0                0   \n",
      "2            0  nsubj     VERB                    0                0   \n",
      "3            0   amod     NOUN                    0                0   \n",
      "4            0  punct     NOUN                    0                0   \n",
      "\n",
      "   ends_with_ment  unc_scope_label  \n",
      "0               0                0  \n",
      "1               0                0  \n",
      "2               0                0  \n",
      "3               0                0  \n",
      "4               0                0  \n",
      "---------------------------------------\n",
      "\n",
      "\n",
      "Document ID: 20331067\n",
      "Sentence 3:\n",
      "--------------------------------------------------\n",
      "WORD            NEG_CUE  NEG_SCOPE  UNC_CUE  UNC_SCOPE\n",
      "--------------------------------------------------\n",
      "no              1        0          0        0\n",
      "intervencions   0        1          0        0\n",
      "quirurgiques    0        1          0        0\n",
      "ni              0        1          0        0\n",
      "altres          0        1          0        0\n",
      "antecedents     0        1          0        0\n",
      "patologics      0        1          0        0\n",
      ".               0        0          0        0\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries and functions\n",
    "import json\n",
    "import spacy\n",
    "from collections import defaultdict\n",
    "import re\n",
    "import pandas as pd\n",
    "from preprocessing import df_svm_neg_test, df_svm_neg_train, df_svm_neg_test, df_svm_unc_train, df_svm_unc_test, df_crf_neg_train, df_crf_neg_test, df_crf_unc_train, df_crf_unc_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CUE DETECTION USING SVM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First of all, we'll need to vectorize:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from joblib import dump\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "\n",
    "def prepare_dataframe_for_svm(df, label_col):\n",
    "    drop_cols = [\"sentence_id\", \"token_id\", label_col]\n",
    "    feature_dicts = df.drop(columns=drop_cols).to_dict(orient=\"records\")\n",
    "    labels = df[label_col].tolist()\n",
    "\n",
    "    vectorizer = DictVectorizer(sparse=True)\n",
    "    X = vectorizer.fit_transform(feature_dicts)\n",
    "    y = labels\n",
    "\n",
    "    return X, y, vectorizer\n",
    "\n",
    "\n",
    "def train_and_evaluate_svm(df_train, df_test, label_col, model_name):\n",
    "    X_train, y_train, vec = prepare_dataframe_for_svm(df_train, label_col)\n",
    "    X_test = vec.transform(df_test.drop(columns=[\"sentence_id\", \"token_id\", label_col]).to_dict(orient=\"records\"))\n",
    "    y_test = df_test[label_col].tolist()\n",
    "\n",
    "    pipeline = Pipeline([\n",
    "        (\"svm\", LinearSVC(class_weight=\"balanced\", max_iter=5000))\n",
    "    ])\n",
    "\n",
    "    pipeline.fit(X_train, y_train)\n",
    "    y_pred = pipeline.predict(X_test)\n",
    "\n",
    "    print(f\"\\n--- Evaluation for {model_name} ---\")\n",
    "    print(classification_report(y_test, y_pred, digits=3))\n",
    "\n",
    "    # Save both the model and the vectorizer\n",
    "    dump(pipeline, f\"{model_name}.joblib\")\n",
    "    dump(vec, f\"{model_name}_vectorizer.joblib\")\n",
    "    return pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVM for negation cue detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Agusl\\anaconda3\\Lib\\site-packages\\sklearn\\svm\\_classes.py:32: FutureWarning: The default value of `dual` will change from `True` to `'auto'` in 1.5. Set the value of `dual` explicitly to suppress the warning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m neg_cue_model \u001b[38;5;241m=\u001b[39m train_and_evaluate_svm(\n\u001b[0;32m      2\u001b[0m     df_train\u001b[38;5;241m=\u001b[39mdf_svm_neg_train,\n\u001b[0;32m      3\u001b[0m     df_test\u001b[38;5;241m=\u001b[39mdf_svm_neg_test,\n\u001b[0;32m      4\u001b[0m     label_col\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mneg_cue_label\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m      5\u001b[0m     model_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msvm_negation_cue\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m      6\u001b[0m )\n",
      "Cell \u001b[1;32mIn[4], line 30\u001b[0m, in \u001b[0;36mtrain_and_evaluate_svm\u001b[1;34m(df_train, df_test, label_col, model_name)\u001b[0m\n\u001b[0;32m     24\u001b[0m y_test \u001b[38;5;241m=\u001b[39m df_test[label_col]\u001b[38;5;241m.\u001b[39mtolist()\n\u001b[0;32m     26\u001b[0m pipeline \u001b[38;5;241m=\u001b[39m Pipeline([\n\u001b[0;32m     27\u001b[0m     (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msvm\u001b[39m\u001b[38;5;124m\"\u001b[39m, LinearSVC(class_weight\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbalanced\u001b[39m\u001b[38;5;124m\"\u001b[39m, max_iter\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5000\u001b[39m))\n\u001b[0;32m     28\u001b[0m ])\n\u001b[1;32m---> 30\u001b[0m pipeline\u001b[38;5;241m.\u001b[39mfit(X_train, y_train)\n\u001b[0;32m     31\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m pipeline\u001b[38;5;241m.\u001b[39mpredict(X_test)\n\u001b[0;32m     33\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m--- Evaluation for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m ---\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\Agusl\\anaconda3\\Lib\\site-packages\\sklearn\\base.py:1151\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1144\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1146\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1147\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1148\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1149\u001b[0m     )\n\u001b[0;32m   1150\u001b[0m ):\n\u001b[1;32m-> 1151\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fit_method(estimator, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\Agusl\\anaconda3\\Lib\\site-packages\\sklearn\\pipeline.py:420\u001b[0m, in \u001b[0;36mPipeline.fit\u001b[1;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[0;32m    418\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_final_estimator \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpassthrough\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    419\u001b[0m         fit_params_last_step \u001b[38;5;241m=\u001b[39m fit_params_steps[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msteps[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m][\u001b[38;5;241m0\u001b[39m]]\n\u001b[1;32m--> 420\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_final_estimator\u001b[38;5;241m.\u001b[39mfit(Xt, y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_params_last_step)\n\u001b[0;32m    422\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\Agusl\\anaconda3\\Lib\\site-packages\\sklearn\\base.py:1151\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1144\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1146\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1147\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1148\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1149\u001b[0m     )\n\u001b[0;32m   1150\u001b[0m ):\n\u001b[1;32m-> 1151\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fit_method(estimator, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\Agusl\\anaconda3\\Lib\\site-packages\\sklearn\\svm\\_classes.py:315\u001b[0m, in \u001b[0;36mLinearSVC.fit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    309\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclasses_ \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39munique(y)\n\u001b[0;32m    311\u001b[0m _dual \u001b[38;5;241m=\u001b[39m _validate_dual_parameter(\n\u001b[0;32m    312\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdual, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloss, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpenalty, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmulti_class, X\n\u001b[0;32m    313\u001b[0m )\n\u001b[1;32m--> 315\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcoef_, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mintercept_, n_iter_ \u001b[38;5;241m=\u001b[39m _fit_liblinear(\n\u001b[0;32m    316\u001b[0m     X,\n\u001b[0;32m    317\u001b[0m     y,\n\u001b[0;32m    318\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mC,\n\u001b[0;32m    319\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfit_intercept,\n\u001b[0;32m    320\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mintercept_scaling,\n\u001b[0;32m    321\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclass_weight,\n\u001b[0;32m    322\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpenalty,\n\u001b[0;32m    323\u001b[0m     _dual,\n\u001b[0;32m    324\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose,\n\u001b[0;32m    325\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_iter,\n\u001b[0;32m    326\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtol,\n\u001b[0;32m    327\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrandom_state,\n\u001b[0;32m    328\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmulti_class,\n\u001b[0;32m    329\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloss,\n\u001b[0;32m    330\u001b[0m     sample_weight\u001b[38;5;241m=\u001b[39msample_weight,\n\u001b[0;32m    331\u001b[0m )\n\u001b[0;32m    332\u001b[0m \u001b[38;5;66;03m# Backward compatibility: _fit_liblinear is used both by LinearSVC/R\u001b[39;00m\n\u001b[0;32m    333\u001b[0m \u001b[38;5;66;03m# and LogisticRegression but LogisticRegression sets a structured\u001b[39;00m\n\u001b[0;32m    334\u001b[0m \u001b[38;5;66;03m# `n_iter_` attribute with information about the underlying OvR fits\u001b[39;00m\n\u001b[0;32m    335\u001b[0m \u001b[38;5;66;03m# while LinearSVC/R only reports the maximum value.\u001b[39;00m\n\u001b[0;32m    336\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_iter_ \u001b[38;5;241m=\u001b[39m n_iter_\u001b[38;5;241m.\u001b[39mmax()\u001b[38;5;241m.\u001b[39mitem()\n",
      "File \u001b[1;32mc:\\Users\\Agusl\\anaconda3\\Lib\\site-packages\\sklearn\\svm\\_base.py:1222\u001b[0m, in \u001b[0;36m_fit_liblinear\u001b[1;34m(X, y, C, fit_intercept, intercept_scaling, class_weight, penalty, dual, verbose, max_iter, tol, random_state, multi_class, loss, epsilon, sample_weight)\u001b[0m\n\u001b[0;32m   1219\u001b[0m sample_weight \u001b[38;5;241m=\u001b[39m _check_sample_weight(sample_weight, X, dtype\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mfloat64)\n\u001b[0;32m   1221\u001b[0m solver_type \u001b[38;5;241m=\u001b[39m _get_liblinear_solver_type(multi_class, penalty, loss, dual)\n\u001b[1;32m-> 1222\u001b[0m raw_coef_, n_iter_ \u001b[38;5;241m=\u001b[39m liblinear\u001b[38;5;241m.\u001b[39mtrain_wrap(\n\u001b[0;32m   1223\u001b[0m     X,\n\u001b[0;32m   1224\u001b[0m     y_ind,\n\u001b[0;32m   1225\u001b[0m     sp\u001b[38;5;241m.\u001b[39misspmatrix(X),\n\u001b[0;32m   1226\u001b[0m     solver_type,\n\u001b[0;32m   1227\u001b[0m     tol,\n\u001b[0;32m   1228\u001b[0m     bias,\n\u001b[0;32m   1229\u001b[0m     C,\n\u001b[0;32m   1230\u001b[0m     class_weight_,\n\u001b[0;32m   1231\u001b[0m     max_iter,\n\u001b[0;32m   1232\u001b[0m     rnd\u001b[38;5;241m.\u001b[39mrandint(np\u001b[38;5;241m.\u001b[39miinfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mi\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mmax),\n\u001b[0;32m   1233\u001b[0m     epsilon,\n\u001b[0;32m   1234\u001b[0m     sample_weight,\n\u001b[0;32m   1235\u001b[0m )\n\u001b[0;32m   1236\u001b[0m \u001b[38;5;66;03m# Regarding rnd.randint(..) in the above signature:\u001b[39;00m\n\u001b[0;32m   1237\u001b[0m \u001b[38;5;66;03m# seed for srand in range [0..INT_MAX); due to limitations in Numpy\u001b[39;00m\n\u001b[0;32m   1238\u001b[0m \u001b[38;5;66;03m# on 32-bit platforms, we can't get to the UINT_MAX limit that\u001b[39;00m\n\u001b[0;32m   1239\u001b[0m \u001b[38;5;66;03m# srand supports\u001b[39;00m\n\u001b[0;32m   1240\u001b[0m n_iter_max \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmax\u001b[39m(n_iter_)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "neg_cue_model = train_and_evaluate_svm(\n",
    "    df_train=df_svm_neg_train,\n",
    "    df_test=df_svm_neg_test,\n",
    "    label_col=\"neg_cue_label\",\n",
    "    model_name=\"svm_negation_cue\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVM for uncertainty cue detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Agusl\\anaconda3\\Lib\\site-packages\\sklearn\\svm\\_classes.py:32: FutureWarning: The default value of `dual` will change from `True` to `'auto'` in 1.5. Set the value of `dual` explicitly to suppress the warning.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Agusl\\anaconda3\\Lib\\site-packages\\sklearn\\svm\\_base.py:1242: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Evaluation for svm_uncertainty_cue ---\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      1.000     0.931     0.964    251284\n",
      "           1      0.038     0.987     0.072       686\n",
      "\n",
      "    accuracy                          0.931    251970\n",
      "   macro avg      0.519     0.959     0.518    251970\n",
      "weighted avg      0.997     0.931     0.962    251970\n",
      "\n"
     ]
    }
   ],
   "source": [
    "unc_cue_model = train_and_evaluate_svm(\n",
    "    df_train=df_svm_unc_train,\n",
    "    df_test=df_svm_unc_test,\n",
    "    label_col=\"unc_cue_label\",\n",
    "    model_name=\"svm_uncertainty_cue\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    2744\n",
      "1     686\n",
      "Name: unc_cue_label, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "def balance_training_data(df, label_col, neg_ratio=4, seed=42):\n",
    "    positives = df[df[label_col] == 1]\n",
    "    negatives = df[df[label_col] == 0].sample(n=len(positives) * neg_ratio, random_state=seed)\n",
    "    df_balanced = pd.concat([positives, negatives]).sample(frac=1, random_state=seed).reset_index(drop=True)\n",
    "    return df_balanced\n",
    "\n",
    "df_balanced_unc_train = balance_training_data(df_svm_unc_train, label_col=\"unc_cue_label\", neg_ratio=4)\n",
    "print(df_balanced_unc_train[\"unc_cue_label\"].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Agusl\\anaconda3\\Lib\\site-packages\\sklearn\\svm\\_classes.py:32: FutureWarning: The default value of `dual` will change from `True` to `'auto'` in 1.5. Set the value of `dual` explicitly to suppress the warning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Evaluation for svm_uncertainty_cue_balanced ---\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      1.000     0.929     0.963    251284\n",
      "           1      0.036     0.987     0.070       686\n",
      "\n",
      "    accuracy                          0.929    251970\n",
      "   macro avg      0.518     0.958     0.517    251970\n",
      "weighted avg      0.997     0.929     0.961    251970\n",
      "\n"
     ]
    }
   ],
   "source": [
    "svm_unc_balanced_model = train_and_evaluate_svm(\n",
    "    df_train=df_balanced_unc_train,\n",
    "    df_test=df_svm_unc_test,\n",
    "    label_col=\"unc_cue_label\",\n",
    "    model_name=\"svm_uncertainty_cue_balanced\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SCOPE DETECTION USING CRF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install sklearn-crfsuite"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll use CRF BIO tagging:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**BIO tagging** is a way to label each word in a sentence to show if it is part of a scope (like negation or uncertainty). The labels are:\n",
    "\n",
    "* **B** for the **Beginning** of the scope\n",
    "* **I** for **Inside** the scope\n",
    "* **O** for **Outside** the scope\n",
    "\n",
    "We use BIO tagging to help machine learning models, like **CRFs (Conditional Random Fields)**, understand where a scope starts and ends. For example, if a sentence has a negation like “No tiene fiebre”, BIO tagging shows that “No” is the beginning (**B-SCOPE**) and “tiene fiebre” is inside the scope (**I-SCOPE**), while other words would be labeled **O** if they are not part of it.\n",
    "\n",
    "Using BIO makes it easier for the model to learn patterns and detect complete scopes correctly, not just single words. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence_id</th>\n",
       "      <th>token_id</th>\n",
       "      <th>word</th>\n",
       "      <th>lemma</th>\n",
       "      <th>pos</th>\n",
       "      <th>prefix</th>\n",
       "      <th>suffix</th>\n",
       "      <th>is_punct</th>\n",
       "      <th>is_redacted</th>\n",
       "      <th>dep</th>\n",
       "      <th>head_pos</th>\n",
       "      <th>in_single_word_cues</th>\n",
       "      <th>in_affixal_cues</th>\n",
       "      <th>ends_with_ment</th>\n",
       "      <th>neg_cue_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>SPACE</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>dep</td>\n",
       "      <td>SPACE</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>nº</td>\n",
       "      <td>nº</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>nº</td>\n",
       "      <td>nº</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>det</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>historia</td>\n",
       "      <td>historia</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>his</td>\n",
       "      <td>ria</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>ROOT</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>clinica</td>\n",
       "      <td>clinico</td>\n",
       "      <td>ADJ</td>\n",
       "      <td>cli</td>\n",
       "      <td>ica</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>amod</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>:</td>\n",
       "      <td>:</td>\n",
       "      <td>PUNCT</td>\n",
       "      <td>:</td>\n",
       "      <td>:</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>punct</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>251965</th>\n",
       "      <td>12732</td>\n",
       "      <td>27</td>\n",
       "      <td>)</td>\n",
       "      <td>)</td>\n",
       "      <td>PUNCT</td>\n",
       "      <td>)</td>\n",
       "      <td>)</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>punct</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>251966</th>\n",
       "      <td>12732</td>\n",
       "      <td>28</td>\n",
       "      <td>&amp;</td>\n",
       "      <td>&amp;</td>\n",
       "      <td>CCONJ</td>\n",
       "      <td>&amp;</td>\n",
       "      <td>&amp;</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>dep</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>251967</th>\n",
       "      <td>12732</td>\n",
       "      <td>29</td>\n",
       "      <td>nbsp;ami-v-pee</td>\n",
       "      <td>nbsp;ami-v-pee</td>\n",
       "      <td>NUM</td>\n",
       "      <td>nbs</td>\n",
       "      <td>pee</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>det</td>\n",
       "      <td>NUM</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>251968</th>\n",
       "      <td>12732</td>\n",
       "      <td>30</td>\n",
       "      <td>2/2</td>\n",
       "      <td>2/2</td>\n",
       "      <td>NUM</td>\n",
       "      <td>2/2</td>\n",
       "      <td>2/2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>obj</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>251969</th>\n",
       "      <td>12732</td>\n",
       "      <td>31</td>\n",
       "      <td>lopd</td>\n",
       "      <td>lopd</td>\n",
       "      <td>ADJ</td>\n",
       "      <td>lop</td>\n",
       "      <td>opd</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>amod</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>251970 rows × 15 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        sentence_id  token_id            word           lemma    pos prefix  \\\n",
       "0                 0         0                                  SPACE          \n",
       "1                 1         0              nº              nº   NOUN     nº   \n",
       "2                 1         1        historia        historia   NOUN    his   \n",
       "3                 1         2         clinica         clinico    ADJ    cli   \n",
       "4                 1         3               :               :  PUNCT      :   \n",
       "...             ...       ...             ...             ...    ...    ...   \n",
       "251965        12732        27               )               )  PUNCT      )   \n",
       "251966        12732        28               &               &  CCONJ      &   \n",
       "251967        12732        29  nbsp;ami-v-pee  nbsp;ami-v-pee    NUM    nbs   \n",
       "251968        12732        30             2/2             2/2    NUM    2/2   \n",
       "251969        12732        31            lopd            lopd    ADJ    lop   \n",
       "\n",
       "       suffix  is_punct  is_redacted    dep head_pos  in_single_word_cues  \\\n",
       "0                     0            0    dep    SPACE                    0   \n",
       "1          nº         0            0    det     NOUN                    0   \n",
       "2         ria         0            0   ROOT     NOUN                    0   \n",
       "3         ica         0            0   amod     NOUN                    0   \n",
       "4           :         1            0  punct     NOUN                    0   \n",
       "...       ...       ...          ...    ...      ...                  ...   \n",
       "251965      )         1            0  punct     NOUN                    1   \n",
       "251966      &         1            0    dep     NOUN                    0   \n",
       "251967    pee         0            0    det      NUM                    0   \n",
       "251968    2/2         0            0    obj     NOUN                    0   \n",
       "251969    opd         0            0   amod     NOUN                    0   \n",
       "\n",
       "        in_affixal_cues  ends_with_ment  neg_cue_label  \n",
       "0                     0               0              0  \n",
       "1                     0               0              0  \n",
       "2                     0               0              0  \n",
       "3                     0               0              0  \n",
       "4                     0               0              0  \n",
       "...                 ...             ...            ...  \n",
       "251965                0               0              0  \n",
       "251966                0               0              0  \n",
       "251967                0               0              0  \n",
       "251968                0               0              0  \n",
       "251969                0               0              0  \n",
       "\n",
       "[251970 rows x 15 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_svm_neg_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#   IM GETTING ALL THE NEGATION AND THE UNC CUES FROM TRAIN DATAFRAME, \n",
    "#   maybe it should be done with the svm but i dont how. \n",
    "neg_cues = set()\n",
    "unc_cues = set()\n",
    "for _, row in df_svm_neg_train.iterrows():\n",
    "    if row['neg_cue_label'] == 1:\n",
    "        neg_cues.add(row['word'].lower())\n",
    "\n",
    "for _, row in df_svm_unc_train.iterrows():\n",
    "    if row['unc_cue_label'] == 1:\n",
    "        unc_cues.add(row['word'].lower())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn_crfsuite import CRF\n",
    "from sklearn_crfsuite import metrics\n",
    "\n",
    "def to_bio_labels(labels, label_type=\"SCOPE\"):\n",
    "    # Convert lists of binary labels (0/1) into BIO tagging format for scopes\n",
    "    bio_labels = []\n",
    "    prefix = label_type.upper() + '_SCOPE'  # e.g., NEG_SCOPE or UNC_SCOPE\n",
    "    for sent in labels:\n",
    "        bio = []\n",
    "        prev = 0\n",
    "        for i, tag in enumerate(sent):\n",
    "            if tag == 1:\n",
    "                if i == 0 or prev == 0:\n",
    "                    bio.append(f'B-{prefix}')\n",
    "                else:\n",
    "                    bio.append(f'I-{prefix}')\n",
    "            else:\n",
    "                bio.append('O')\n",
    "            prev = tag\n",
    "        bio_labels.append(bio)\n",
    "    return bio_labels\n",
    "\n",
    "def df_to_crf_format(df, task, neg_cues=neg_cues, unc_cues=unc_cues):\n",
    "    \"\"\"\n",
    "    Convert a DataFrame into a list of feature dictionaries per sentence for CRF input.\n",
    "    Includes original features + contextual features + lexicon-based features.\n",
    "    \n",
    "    Parameters:\n",
    "        df (pd.DataFrame): must contain columns like 'word', 'pos', 'prefix', 'suffix', etc.\n",
    "\n",
    "    Returns:\n",
    "        List of list of feature dicts (one per token, grouped by sentence)\n",
    "    \"\"\"\n",
    "    sentences = []\n",
    "    grouped = df.groupby(\"sentence_id\")\n",
    "\n",
    "    for _, group in grouped:\n",
    "        sentence = []\n",
    "        group = group.reset_index(drop=True)  # Reset index so we can use idx in loop\n",
    "\n",
    "        for idx, row in group.iterrows():\n",
    "            word_lower = row['word'].lower()\n",
    "\n",
    "            features = {\n",
    "                'word.lower()': word_lower,\n",
    "                'word.isupper()': row['word'].isupper(),\n",
    "                'word.istitle()': row['word'].istitle(),\n",
    "                'pos': row['pos'],\n",
    "                'pos_prefix': row['pos'][:2] if isinstance(row['pos'], str) else 'NA',\n",
    "                'prefix': row['prefix'],\n",
    "                'suffix': row['suffix'],\n",
    "                'is_punct': row['is_punct'],\n",
    "                'in_single_word_cues': row['in_single_word_cues'],\n",
    "                'in_affixal_cues': row['in_affixal_cues'],\n",
    "                'ends_with_ment': row['ends_with_ment'],\n",
    "                'has_neg_prefix': word_lower.startswith(('un', 'in', 'non', 'dis')),\n",
    "                'has_neg_suffix': word_lower.endswith(('less', \"n't\")),\n",
    "                'is_modal': word_lower in unc_cues if task == \"UNC\" else word_lower in neg_cues\n",
    "            }\n",
    "    \n",
    "            # dependency features\n",
    "            if 'dep' in row and 'head_word' in row and 'head_pos' in row:\n",
    "                features.update({\n",
    "                    'dep_label': row['dep'],\n",
    "                    'head_word': str(row['head_word']).lower(),\n",
    "                    'head_pos': row['head_pos']\n",
    "                })\n",
    "\n",
    "            # Contextual features: previous and next token\n",
    "            if idx > 0:\n",
    "                prev_row = group.iloc[idx - 1]\n",
    "                features.update({\n",
    "                    '-1:word.lower()': prev_row['word'].lower(),\n",
    "                    '-1:pos': prev_row['pos']\n",
    "                })\n",
    "            else:\n",
    "                features['BOS'] = True  # Beginning of sentence\n",
    "\n",
    "            if idx < len(group) - 1:\n",
    "                next_row = group.iloc[idx + 1]\n",
    "                features.update({\n",
    "                    '+1:word.lower()': next_row['word'].lower(),\n",
    "                    '+1:pos': next_row['pos']\n",
    "                })\n",
    "            else:\n",
    "                features['EOS'] = True  # End of sentence\n",
    "\n",
    "            sentence.append(features)\n",
    "        sentences.append(sentence)\n",
    "\n",
    "    return sentences\n",
    "\n",
    "\n",
    "def df_to_labels(df, label_col):\n",
    "    # Extracts label sequences from the DataFrame, grouped by sentence\n",
    "    label_sequences = []\n",
    "    grouped = df.groupby(\"sentence_id\")\n",
    "    for _, group in grouped:\n",
    "        label_list = group[label_col].tolist()\n",
    "        label_sequences.append(label_list)\n",
    "    return label_sequences\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train + evaluate CRF model\n",
    "def train_and_evaluate_crf(df_train, df_test, label_col):\n",
    "    # Trains and evaluates a CRF model for BIO tagging using specified label column (e.g., 'neg_scope_label')\n",
    "    scope_type = \"NEG\" if \"neg\" in label_col.lower() else \"UNC\"\n",
    "\n",
    "    X_train = df_to_crf_format(df_train, scope_type)\n",
    "    y_train_raw = df_to_labels(df_train, label_col)\n",
    "    y_train = to_bio_labels(y_train_raw, label_type=scope_type)\n",
    "\n",
    "    X_test = df_to_crf_format(df_test, scope_type)\n",
    "    y_test_raw = df_to_labels(df_test, label_col)\n",
    "    y_test = to_bio_labels(y_test_raw, label_type=scope_type)\n",
    "\n",
    "    crf = CRF(algorithm='lbfgs', max_iterations=100, all_possible_transitions=True)\n",
    "    crf.fit(X_train, y_train)\n",
    "    y_pred = crf.predict(X_test)\n",
    "\n",
    "    print(f\"CRF Evaluation for: {label_col.upper()}\")\n",
    "    print(metrics.flat_classification_report(y_test, y_pred))   \n",
    "    \n",
    "    return X_test, y_test, y_pred  # Return these variables for further use\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CRF for negation scope detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CRF Evaluation for: NEG_SCOPE_LABEL\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      " B-NEG_SCOPE       0.97      0.89      0.93      1071\n",
      " I-NEG_SCOPE       0.91      0.79      0.84      2522\n",
      "           O       0.99      1.00      0.99     61938\n",
      "\n",
      "    accuracy                           0.99     65531\n",
      "   macro avg       0.96      0.89      0.92     65531\n",
      "weighted avg       0.99      0.99      0.99     65531\n",
      "\n",
      "\n",
      "--- Sentence 0 (ID 0) ---\n",
      "WORD            TRUE            PRED           \n",
      "---------------------------------------------\n",
      "                O               O              \n",
      "\n",
      "--- Sentence 1 (ID 1) ---\n",
      "WORD            TRUE            PRED           \n",
      "---------------------------------------------\n",
      "nº              O               O              \n",
      "historia        O               O              \n",
      "clinica         O               O              \n",
      ":               O               O              \n",
      "*               O               O              \n",
      "*               O               O              \n",
      "*               O               O              \n",
      "*               O               O              \n",
      "*               O               O              \n",
      "*               O               O              \n",
      "*               O               O              \n",
      "*               O               O              \n",
      "nºepisodi       O               O              \n",
      ":               O               O              \n",
      "*               O               O              \n",
      "*               O               O              \n",
      "*               O               O              \n",
      "*               O               O              \n",
      "*               O               O              \n",
      "*               O               O              \n",
      "*               O               O              \n",
      "*               O               O              \n",
      "sexe            O               O              \n",
      ":               O               O              \n",
      "dona            O               O              \n",
      "data            O               O              \n",
      "de              O               O              \n",
      "naixement       O               O              \n",
      ":               O               O              \n",
      "12.05.1977      O               O              \n",
      "edat            O               O              \n",
      ":               O               O              \n",
      "42              O               O              \n",
      "anys            O               O              \n",
      "procedencia     O               O              \n",
      "aguts           O               O              \n",
      "servei          O               O              \n",
      "obstetricia     O               O              \n",
      "data            O               O              \n",
      "d'ingres        O               O              \n",
      "27.09.2019      O               O              \n",
      "data            O               O              \n",
      "d'alta          O               O              \n",
      "01.10.2019      O               O              \n",
      "13:00:00        O               O              \n",
      "ates            O               O              \n",
      "per             O               O              \n",
      "*               O               O              \n",
      "*               O               O              \n",
      "*               O               O              \n",
      "*               O               O              \n",
      "*               O               O              \n",
      "*               O               O              \n",
      "*               O               O              \n",
      "*               O               O              \n",
      "*               O               O              \n",
      "*               O               O              \n",
      "*               O               O              \n",
      "*               O               O              \n",
      "*               O               O              \n",
      "*               O               O              \n",
      "*               O               O              \n",
      "*               O               O              \n",
      "*               O               O              \n",
      ",               O               O              \n",
      "*               O               O              \n",
      "*               O               O              \n",
      "*               O               O              \n",
      "*               O               O              \n",
      "*               O               O              \n",
      "*               O               O              \n",
      "*               O               O              \n",
      "*               O               O              \n",
      "*               O               O              \n",
      "*               O               O              \n",
      "*               O               O              \n",
      "*               O               O              \n",
      "*               O               O              \n",
      "*               O               O              \n",
      ";               O               O              \n",
      "*               O               O              \n",
      "*               O               O              \n",
      "*               O               O              \n",
      "*               O               O              \n",
      "*               O               O              \n",
      "*               O               O              \n",
      "*               O               O              \n",
      "*               O               O              \n",
      "*               O               O              \n",
      "*               O               O              \n",
      "*               O               O              \n",
      "*               O               O              \n",
      "*               O               O              \n",
      "*               O               O              \n",
      "*               O               O              \n",
      "*               O               O              \n",
      "*               O               O              \n",
      ",               O               O              \n",
      "*               O               O              \n",
      "*               O               O              \n",
      "*               O               O              \n",
      "*               O               O              \n",
      "*               O               O              \n",
      "informe         O               O              \n",
      "d'alta          O               O              \n",
      "d'hospitalitzacio O               O              \n",
      "motiu           O               O              \n",
      "d'ingres        O               O              \n",
      "induccion       O               O              \n",
      "al              O               O              \n",
      "parto           O               O              \n",
      "por             O               O              \n",
      "pequeño         O               O              \n",
      "para            O               O              \n",
      "la              O               O              \n",
      "edad            O               O              \n",
      "gestacional     O               O              \n",
      "(               O               O              \n",
      "peg             O               O              \n",
      ")               O               O              \n",
      "antecedents     O               O              \n",
      "no              O               O              \n",
      "alergias        B-NEG_SCOPE     B-NEG_SCOPE    \n",
      "medicamentosas  I-NEG_SCOPE     I-NEG_SCOPE    \n",
      "conocidas       O               I-NEG_SCOPE    \n",
      "antcededentes   O               I-NEG_SCOPE    \n",
      "medico-quirurgicos O               I-NEG_SCOPE    \n",
      ":               O               O              \n",
      "protesis        O               O              \n",
      "mamaria         O               O              \n",
      ",               O               O              \n",
      "adenoidectomia  O               O              \n",
      "niega           O               O              \n",
      "habitos         B-NEG_SCOPE     B-NEG_SCOPE    \n",
      "toxicos         I-NEG_SCOPE     I-NEG_SCOPE    \n",
      "medicacio       O               O              \n",
      "habitual        O               O              \n",
      "anafranil25     O               O              \n",
      "mg/             O               O              \n",
      "diario          O               O              \n",
      ".               O               O              \n",
      "\n",
      "--- Sentence 2 (ID 2) ---\n",
      "WORD            TRUE            PRED           \n",
      "---------------------------------------------\n",
      "yodocefol       O               O              \n",
      ".               O               O              \n",
      "\n",
      "--- Sentence 3 (ID 3) ---\n",
      "WORD            TRUE            PRED           \n",
      "---------------------------------------------\n",
      "hierro          O               O              \n",
      "oral            O               O              \n",
      ".               O               O              \n",
      "\n",
      "--- Sentence 4 (ID 4) ---\n",
      "WORD            TRUE            PRED           \n",
      "---------------------------------------------\n",
      "ranitidina      O               O              \n",
      "150             O               O              \n",
      "mg              O               O              \n",
      ".               O               O              \n"
     ]
    }
   ],
   "source": [
    "# CRF BIO tagging evaluation for NEGATION scopes\n",
    "X_test_NEG, y_test_NEG, y_pred_NEG = train_and_evaluate_crf(df_crf_neg_train, df_crf_neg_test, \"neg_scope_label\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CRF for uncertainty scope detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CRF Evaluation for: UNC_SCOPE_LABEL\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      " B-UNC_SCOPE       0.89      0.25      0.39       129\n",
      " I-UNC_SCOPE       0.78      0.31      0.44       437\n",
      "           O       0.99      1.00      1.00     64965\n",
      "\n",
      "    accuracy                           0.99     65531\n",
      "   macro avg       0.89      0.52      0.61     65531\n",
      "weighted avg       0.99      0.99      0.99     65531\n",
      "\n",
      "\n",
      "--- Sentence 100 (ID 100) ---\n",
      "WORD            TRUE            PRED           \n",
      "---------------------------------------------\n",
      "cardiovascular  O               O              \n",
      ":               O               O              \n",
      "auscultacion    O               O              \n",
      "cardiaca        O               O              \n",
      "con             O               O              \n",
      "tonos           O               O              \n",
      "ritmicos        O               O              \n",
      "y               O               O              \n",
      "sin             O               O              \n",
      "soplos          O               O              \n",
      ";               O               O              \n",
      "no              O               O              \n",
      "edemas          O               O              \n",
      "en              O               O              \n",
      "miembros        O               O              \n",
      "inferiores      O               O              \n",
      "ni              O               O              \n",
      "lesiones        O               O              \n",
      "cutaneas        O               O              \n",
      ".               O               O              \n",
      "\n",
      "--- Sentence 101 (ID 101) ---\n",
      "WORD            TRUE            PRED           \n",
      "---------------------------------------------\n",
      "respiratorio    O               O              \n",
      ":               O               O              \n",
      "auscultacion    O               O              \n",
      "respiratoria    O               O              \n",
      "con             O               O              \n",
      "murmullo        O               O              \n",
      "vesicular       O               O              \n",
      "conservado      O               O              \n",
      ",               O               O              \n",
      "sin             O               O              \n",
      "ruidos          O               O              \n",
      "patologicos     O               O              \n",
      "añadidos        O               O              \n",
      ".               O               O              \n",
      "\n",
      "--- Sentence 102 (ID 102) ---\n",
      "WORD            TRUE            PRED           \n",
      "---------------------------------------------\n",
      "abdomen         O               O              \n",
      ":               O               O              \n",
      "no              O               O              \n",
      "distendido      O               O              \n",
      ",               O               O              \n",
      "blando          O               O              \n",
      "y               O               O              \n",
      "depresible      O               O              \n",
      ",               O               O              \n",
      "no              O               O              \n",
      "doloroso        O               O              \n",
      "a               O               O              \n",
      "la              O               O              \n",
      "palpacion       O               O              \n",
      ".               O               O              \n",
      "\n",
      "--- Sentence 103 (ID 103) ---\n",
      "WORD            TRUE            PRED           \n",
      "---------------------------------------------\n",
      "no              O               O              \n",
      "se              O               O              \n",
      "palpan          O               O              \n",
      "masas           O               O              \n",
      "ni              O               O              \n",
      "megalias        O               O              \n",
      ",               O               O              \n",
      "ni              O               O              \n",
      "tampoco         O               O              \n",
      "globo           O               O              \n",
      "vesical         O               O              \n",
      ".               O               O              \n",
      "\n",
      "--- Sentence 104 (ID 104) ---\n",
      "WORD            TRUE            PRED           \n",
      "---------------------------------------------\n",
      "no              O               O              \n",
      "signos          O               O              \n",
      "de              O               O              \n",
      "irritacion      O               O              \n",
      "peritoneal      O               O              \n",
      ".               O               O              \n",
      "\n",
      "--- Sentence 105 (ID 105) ---\n",
      "WORD            TRUE            PRED           \n",
      "---------------------------------------------\n",
      "puñopercusion   O               O              \n",
      "lumbar          O               O              \n",
      "bilateral       O               O              \n",
      "negativa        O               O              \n",
      ".               O               O              \n"
     ]
    }
   ],
   "source": [
    "# CRF BIO tagging evaluation for UNCERTAINTY scopes\n",
    "X_test_UNC, y_test_UNC, y_pred_UNC = train_and_evaluate_crf(df_crf_unc_train, df_crf_unc_test, \"unc_scope_label\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall Evaluation (NEG + UNC scopes):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      " B-NEG_SCOPE      0.973     0.890     0.930      1071\n",
      " B-UNC_SCOPE      0.889     0.248     0.388       129\n",
      " I-NEG_SCOPE      0.906     0.787     0.842      2522\n",
      " I-UNC_SCOPE      0.784     0.307     0.441       437\n",
      "\n",
      "   micro avg      0.919     0.746     0.824      4159\n",
      "   macro avg      0.888     0.558     0.650      4159\n",
      "weighted avg      0.910     0.746     0.808      4159\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# TABLE WITH ALL COMBINED EXCLUDING THE OUTOFSCOPE: \n",
    "# Combine the BIO label sequences\n",
    "y_test_all = y_test_NEG + y_test_UNC\n",
    "y_pred_all = y_pred_NEG + y_pred_UNC\n",
    "\n",
    "# Get all labels except 'O'\n",
    "all_labels = set(label for seq in y_test_all for label in seq if label != 'O')\n",
    "all_labels = sorted(all_labels)\n",
    "\n",
    "# Print overall evaluation\n",
    "print(\"Overall Evaluation (NEG + UNC scopes):\")\n",
    "print(metrics.flat_classification_report(\n",
    "    y_test_all, y_pred_all, labels=all_labels, digits=3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Printing CRF Predictions vs. True labels\n",
    "We display sample sentences with predicted and true BIO tags shown as `[true_tag|pred_tag]` next to each word, only if at least one tag isn't `'O'`. This highlights where the model correctly identifies scopes or makes mistakes, helping us quickly assess its performance on meaningful cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_crf_predictions(df, y_true, y_pred, sentence_idx=0):\n",
    "    \"\"\"\n",
    "    Print the whole sentence in one line, adding true and predicted BIO tags in brackets next to tokens\n",
    "    only if the tags are not 'O'.\n",
    "    Format per token: word[true_tag|pred_tag] if either tag != 'O', else just word.\n",
    "    \"\"\"\n",
    "    grouped = df.groupby(\"sentence_id\")\n",
    "    sentence_ids = list(grouped.groups.keys())\n",
    "\n",
    "    if sentence_idx >= len(sentence_ids):\n",
    "        print(f\"Invalid sentence index {sentence_idx}. Max allowed: {len(sentence_ids) - 1}\")\n",
    "        return\n",
    "\n",
    "    sentence_id = sentence_ids[sentence_idx]\n",
    "    sentence_df = grouped.get_group(sentence_id).reset_index(drop=True)\n",
    "\n",
    "    output_tokens = []\n",
    "    for i, row in sentence_df.iterrows():\n",
    "        word = row['word']\n",
    "        true_label = y_true[sentence_idx][i]\n",
    "        pred_label = y_pred[sentence_idx][i]\n",
    "\n",
    "        # Show tags only if either true or pred label is not 'O'\n",
    "        if true_label != 'O' or pred_label != 'O':\n",
    "            display_word = f\"{word} [{true_label}|{pred_label}]\"\n",
    "        else:\n",
    "            display_word = word\n",
    "\n",
    "        output_tokens.append(display_word)\n",
    "\n",
    "    print(f\"\\n--- Sentence ID {sentence_id} ---\")\n",
    "    print(\" \".join(output_tokens))\n",
    "\n",
    "for i in range(5):\n",
    "    print_crf_predictions(df_crf_neg_test, y_test_NEG, y_pred_NEG, sentence_idx=i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Sentence ID 1 ---\n",
      "nº historia clinica : * * * * * * * * nºepisodi : * * * * * * * * sexe : dona data de naixement : 12.05.1977 edat : 42 anys procedencia aguts servei obstetricia data d'ingres 27.09.2019 data d'alta 01.10.2019 13:00:00 ates per * * * * * * * * * * * * * * * * * , * * * * * * * * * * * * * * ; * * * * * * * * * * * * * * * * * , * * * * * informe d'alta d'hospitalitzacio motiu d'ingres induccion al parto por pequeño para la edad gestacional ( peg ) antecedents no alergias [B-NEG_SCOPE|B-NEG_SCOPE] medicamentosas [I-NEG_SCOPE|I-NEG_SCOPE] conocidas [O|I-NEG_SCOPE] antcededentes [O|I-NEG_SCOPE] medico-quirurgicos [O|I-NEG_SCOPE] : protesis mamaria , adenoidectomia niega habitos [B-NEG_SCOPE|B-NEG_SCOPE] toxicos [I-NEG_SCOPE|I-NEG_SCOPE] medicacio habitual anafranil25 mg/ diario .\n",
      "\n",
      "--- Sentence ID 10 ---\n",
      "vih [B-NEG_SCOPE|B-NEG_SCOPE] negativo .\n",
      "\n",
      "--- Sentence ID 11 ---\n",
      "serologias : rubeola no inmune [B-NEG_SCOPE|B-NEG_SCOPE] , toxoplasma no immune [B-NEG_SCOPE|B-NEG_SCOPE] , [O|I-NEG_SCOPE] lues [B-NEG_SCOPE|I-NEG_SCOPE] vih [I-NEG_SCOPE|I-NEG_SCOPE] , [I-NEG_SCOPE|I-NEG_SCOPE] vhb [I-NEG_SCOPE|I-NEG_SCOPE] y [I-NEG_SCOPE|I-NEG_SCOPE] vhc [I-NEG_SCOPE|I-NEG_SCOPE] negativos .\n",
      "\n",
      "--- Sentence ID 12 ---\n",
      "- triple screening : 1/14045 down i 1/60540 edwards - riesgo de pe : bajo riesgo - o'sullivan : 216mg / dl - sgb [O|B-NEG_SCOPE] negativo - eco 1º t : crl:60 tn:1.7 - 2º t : morfologia normal .\n",
      "\n",
      "--- Sentence ID 27 ---\n",
      "procedimiento sin incidencias [B-NEG_SCOPE|B-NEG_SCOPE] .\n",
      "\n",
      "--- Sentence ID 49 ---\n",
      "antecedents alergias no refiere [B-NEG_SCOPE|B-NEG_SCOPE] alergias [I-NEG_SCOPE|I-NEG_SCOPE] medicamentosas [I-NEG_SCOPE|I-NEG_SCOPE] conocidas [I-NEG_SCOPE|I-NEG_SCOPE] .\n",
      "\n",
      "--- Sentence ID 50 ---\n",
      "antecedenes personales niega habitos [B-NEG_SCOPE|B-NEG_SCOPE] toxicos [I-NEG_SCOPE|I-NEG_SCOPE] .\n",
      "\n",
      "--- Sentence ID 55 ---\n",
      "* * serologies : hbsag [B-NEG_SCOPE|O] negatiu , hbsac positiu , hbcac [B-NEG_SCOPE|O] negatiu , vhc [B-NEG_SCOPE|B-NEG_SCOPE] negativa -antecedentes its : * * uretritis gonococica en mayo 2017 , realizo tratamientio .\n",
      "\n",
      "--- Sentence ID 61 ---\n",
      "ultimos dos años sin episodios [B-NEG_SCOPE|B-NEG_SCOPE] .\n",
      "\n",
      "--- Sentence ID 62 ---\n",
      "antecedentes quirurgicos : no refiere [B-NEG_SCOPE|B-NEG_SCOPE] .\n"
     ]
    }
   ],
   "source": [
    "shown = 0\n",
    "max_to_show = 10\n",
    "\n",
    "for i in range(len(y_test_NEG)):\n",
    "    if any(tag != 'O' for tag in y_test_NEG[i]) or any(tag != 'O' for tag in y_pred_NEG[i]):\n",
    "        print_crf_predictions(df_crf_neg_test, y_test_NEG, y_pred_NEG, sentence_idx=i)\n",
    "        shown += 1\n",
    "        if shown >= max_to_show:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Sentence ID 138 ---\n",
      "dado cuadro clinico y analitico que podria ser [B-UNC_SCOPE|O] compatible [I-UNC_SCOPE|O] con [I-UNC_SCOPE|O] sd [I-UNC_SCOPE|O] mononucleosido [I-UNC_SCOPE|O] se amplia estudio con determinacion de pcr de parvovirus b19 , toxoplasma y vhh-6 , con resultados pendientes .\n",
      "\n",
      "--- Sentence ID 140 ---\n",
      "orientacio diagnostica b34.9 infeccio virica no especificada neutropenia autolimitada fiebre de origen desconocido , posible viriasis [B-UNC_SCOPE|B-UNC_SCOPE] . [O|I-UNC_SCOPE]\n",
      "\n",
      "--- Sentence ID 156 ---\n",
      "antecedentes patologicos - elevacion de transaminasas posiblemente secundario [B-UNC_SCOPE|O] a [I-UNC_SCOPE|O] consumo [I-UNC_SCOPE|O] enolico [I-UNC_SCOPE|O] . [I-UNC_SCOPE|O]\n",
      "\n",
      "--- Sentence ID 188 ---\n",
      "hipoestesia simetrica en mmii hasta abdomen ( sin claro nivel [B-UNC_SCOPE|O] ) [I-UNC_SCOPE|O] .\n",
      "\n",
      "--- Sentence ID 193 ---\n",
      "atrofia hipotenar y posible atrofia [B-UNC_SCOPE|O] interoseos [I-UNC_SCOPE|O] ( paciente refiere que siempre ha permanecido asi ) .\n",
      "\n",
      "--- Sentence ID 228 ---\n",
      "por las caracteristicas encontradas , sugiere origen [B-UNC_SCOPE|O] inmunomediado [I-UNC_SCOPE|O] . [I-UNC_SCOPE|O]\n",
      "\n",
      "--- Sentence ID 257 ---\n",
      "se solicita analitica de ingreso que muestra patron de enolismo cronico ( macrocitosis y elevacion de ggt ) , un patron compatible con hipotiroidismo [B-UNC_SCOPE|O] subclinico [I-UNC_SCOPE|O] autoimune [I-UNC_SCOPE|O] ( por el que se realiza itc a endocrinologia que recomiendan control analitico en 6 meses ) ; con perfil autoimune ( ana , anca , sd antifosflipido ) negativo y serologias viricas negativas .\n",
      "\n",
      "--- Sentence ID 258 ---\n",
      "se solicita estudio neurofisiologico que muestra patron [O|B-UNC_SCOPE] de [O|I-UNC_SCOPE] polineuropatia [O|I-UNC_SCOPE] sensitiva [O|I-UNC_SCOPE] de [O|I-UNC_SCOPE] predominio [O|I-UNC_SCOPE] en [O|I-UNC_SCOPE] eess [O|I-UNC_SCOPE] con [O|I-UNC_SCOPE] componente [O|I-UNC_SCOPE] radicular [O|I-UNC_SCOPE] añadido [O|I-UNC_SCOPE] , [O|I-UNC_SCOPE] que por las caracteristicas encontradas , sugiere origen [B-UNC_SCOPE|O] inmunomediado [I-UNC_SCOPE|O] ; [I-UNC_SCOPE|O] sin poderse definir un patron axonal o desmielinizante .\n",
      "\n",
      "--- Sentence ID 260 ---\n",
      "asi pues , ante dichos hallazgos se orienta como [B-UNC_SCOPE|B-UNC_SCOPE] cuadro [I-UNC_SCOPE|I-UNC_SCOPE] de [I-UNC_SCOPE|I-UNC_SCOPE] polirradiculoneuropatia [I-UNC_SCOPE|I-UNC_SCOPE] inflamatoria [I-UNC_SCOPE|I-UNC_SCOPE] aguda [I-UNC_SCOPE|I-UNC_SCOPE] y se decide administrar immungolobulinas endovenosas 5 dias , pese a que la emg no era concluyente ; presentando posteriormente la paciente mejoria progresiva de la clinica .\n",
      "\n",
      "--- Sentence ID 265 ---\n",
      "si bien es cierto que la paciente mejoro tras el inicio de inmunoglobulinas , tambien es cierto que es mas que probable que [B-UNC_SCOPE|O] su [I-UNC_SCOPE|O] enolismo [I-UNC_SCOPE|O] cronico [I-UNC_SCOPE|O] e [I-UNC_SCOPE|O] hipovitaminosis [I-UNC_SCOPE|O] hayan [I-UNC_SCOPE|O] tenido [I-UNC_SCOPE|O] un [I-UNC_SCOPE|O] papel [I-UNC_SCOPE|O] en [I-UNC_SCOPE|O] la [I-UNC_SCOPE|O] clinica [I-UNC_SCOPE|O] de [I-UNC_SCOPE|O] la [I-UNC_SCOPE|O] paciente [I-UNC_SCOPE|O] . [I-UNC_SCOPE|O]\n"
     ]
    }
   ],
   "source": [
    "shown = 0\n",
    "max_to_show = 10\n",
    "\n",
    "for i in range(len(y_test_UNC)):\n",
    "    if any(tag != 'O' for tag in y_test_UNC[i]) or any(tag != 'O' for tag in y_pred_UNC[i]):\n",
    "        print_crf_predictions(df_crf_neg_test, y_test_UNC, y_pred_UNC, sentence_idx=i)\n",
    "        shown += 1\n",
    "        if shown >= max_to_show:\n",
    "            break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
