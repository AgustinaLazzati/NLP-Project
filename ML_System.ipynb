{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fundamentals of Natural Language Processing\n",
    "# Negation and Uncertainty Detection using a Machine-Learning Based Approach\n",
    "\n",
    "*Authors:*\n",
    "\n",
    "> *Anna Blanco, Agustina Lazzati, Stanislav Bultaskii, Queralt Salvadó*\n",
    "\n",
    "*Aims:*\n",
    "> Our goal is to train various Machine Learning based models for each of the two sub-tasks (detection of negation and uncertainty signals, and detection of the negation and uncertainty scopes). In order to do so, we followed the implementation method described by *Enger, Velldal, and Øvrelid (2017)*, which employs a maximum-margin approach for negation detection. However, for our particular application, we also included uncertainty cues and scope detection."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*References:* \n",
    "<br>\n",
    "> Enger, M., Velldal, E., & Øvrelid, L. (2017). *An open-source tool for negation detection: A maximum-margin approach*. Proceedings of the Workshop on Computational Semantics Beyond Events and Roles (SemBEaR), 64–69."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can erase this if you want but the thing is that we need to use the environment that queralt did. You need to write some commands to have the nlp_project (Python) as we have specific libraries. \n",
    "\n",
    "I did that and in the preprocessing it worked but here in order to work I had to run this command above, if it is not needed just avoid them!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "# Check installed models\n",
    "print(spacy.util.get_installed_models())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting es-core-news-sm==3.8.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/es_core_news_sm-3.8.0/es_core_news_sm-3.8.0-py3-none-any.whl (12.9 MB)\n",
      "     ---------------------------------------- 0.0/12.9 MB ? eta -:--:--\n",
      "     ------------------ --------------------- 6.0/12.9 MB 37.0 MB/s eta 0:00:01\n",
      "     --------------------------------------- 12.9/12.9 MB 36.7 MB/s eta 0:00:00\n",
      "Installing collected packages: es-core-news-sm\n",
      "Successfully installed es-core-news-sm-3.8.0\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('es_core_news_sm')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.0.1 -> 25.1.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "#!python -m spacy download es_core_news_sm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries and functions\n",
    "import json\n",
    "import spacy\n",
    "from collections import defaultdict\n",
    "import re\n",
    "import pandas as pd\n",
    "from preprocessing import df_svm_neg_test, df_svm_neg_train, df_svm_unc_train, df_svm_unc_test, df_crf_neg_train, df_crf_neg_test, df_crf_unc_train, df_crf_unc_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CUE DETECTION USING SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from joblib import dump\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "\n",
    "def prepare_dataframe_for_svm(df, label_col):\n",
    "    drop_cols = [\"sentence_id\", \"token_id\", label_col]\n",
    "    feature_dicts = df.drop(columns=drop_cols).to_dict(orient=\"records\")\n",
    "    labels = df[label_col].tolist()\n",
    "\n",
    "    vectorizer = DictVectorizer(sparse=True)\n",
    "    X = vectorizer.fit_transform(feature_dicts)\n",
    "    y = labels\n",
    "\n",
    "    return X, y, vectorizer\n",
    "\n",
    "\n",
    "def train_and_evaluate_svm(df_train, df_test, label_col, model_name):\n",
    "    X_train, y_train, vec = prepare_dataframe_for_svm(df_train, label_col)\n",
    "    X_test = vec.transform(df_test.drop(columns=[\"sentence_id\", \"token_id\", label_col]).to_dict(orient=\"records\"))\n",
    "    y_test = df_test[label_col].tolist()\n",
    "\n",
    "    pipeline = Pipeline([\n",
    "        (\"svm\", LinearSVC(class_weight=\"balanced\", max_iter=5000))\n",
    "    ])\n",
    "\n",
    "    pipeline.fit(X_train, y_train)\n",
    "    y_pred = pipeline.predict(X_test)\n",
    "\n",
    "    print(f\"\\n--- Evaluation for {model_name} ---\")\n",
    "    print(classification_report(y_test, y_pred, digits=3))\n",
    "\n",
    "    # Save both the model and the vectorizer\n",
    "    dump(pipeline, f\"{model_name}.joblib\")\n",
    "    dump(vec, f\"{model_name}_vectorizer.joblib\")\n",
    "    return pipeline\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVM for negation cue detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Evaluation for svm_negation_cue ---\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      1.000     0.998     0.999     64399\n",
      "           1      0.916     0.996     0.954      1132\n",
      "\n",
      "    accuracy                          0.998     65531\n",
      "   macro avg      0.958     0.997     0.977     65531\n",
      "weighted avg      0.998     0.998     0.998     65531\n",
      "\n"
     ]
    }
   ],
   "source": [
    "neg_cue_model = train_and_evaluate_svm(\n",
    "    df_train=df_svm_neg_train,\n",
    "    df_test=df_svm_neg_test,\n",
    "    label_col=\"neg_cue_label\",\n",
    "    model_name=\"svm_negation_cue\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVM for uncertainty cue detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Evaluation for svm_uncertainty_cue ---\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      1.000     0.931     0.964    251284\n",
      "           1      0.038     0.987     0.072       686\n",
      "\n",
      "    accuracy                          0.931    251970\n",
      "   macro avg      0.519     0.959     0.518    251970\n",
      "weighted avg      0.997     0.931     0.962    251970\n",
      "\n"
     ]
    }
   ],
   "source": [
    "unc_cue_model = train_and_evaluate_svm(\n",
    "    df_train=df_svm_unc_train,\n",
    "    df_test=df_svm_unc_test,\n",
    "    label_col=\"unc_cue_label\",\n",
    "    model_name=\"svm_uncertainty_cue\"\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unc_cue_label\n",
      "0    2744\n",
      "1     686\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "def balance_training_data(df, label_col, neg_ratio=4, seed=42):\n",
    "    positives = df[df[label_col] == 1]\n",
    "    negatives = df[df[label_col] == 0].sample(n=len(positives) * neg_ratio, random_state=seed)\n",
    "    df_balanced = pd.concat([positives, negatives]).sample(frac=1, random_state=seed).reset_index(drop=True)\n",
    "    return df_balanced\n",
    "\n",
    "df_balanced_unc_train = balance_training_data(df_svm_unc_train, label_col=\"unc_cue_label\", neg_ratio=4)\n",
    "print(df_balanced_unc_train[\"unc_cue_label\"].value_counts())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Evaluation for svm_uncertainty_cue_balanced ---\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      1.000     0.929     0.963    251284\n",
      "           1      0.036     0.987     0.070       686\n",
      "\n",
      "    accuracy                          0.929    251970\n",
      "   macro avg      0.518     0.958     0.517    251970\n",
      "weighted avg      0.997     0.929     0.961    251970\n",
      "\n"
     ]
    }
   ],
   "source": [
    "svm_unc_balanced_model = train_and_evaluate_svm(\n",
    "    df_train=df_balanced_unc_train,\n",
    "    df_test=df_svm_unc_test,\n",
    "    label_col=\"unc_cue_label\",\n",
    "    model_name=\"svm_uncertainty_cue_balanced\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SCOPE DETECTION USING CRF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install sklearn-crfsuite"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll use CRF BIO tagging:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**BIO tagging** is a way to label each word in a sentence to show if it is part of a scope (like negation or uncertainty). The labels are:\n",
    "\n",
    "* **B** for the **Beginning** of the scope\n",
    "* **I** for **Inside** the scope\n",
    "* **O** for **Outside** the scope\n",
    "\n",
    "We use BIO tagging to help machine learning models, like **CRFs (Conditional Random Fields)**, understand where a scope starts and ends. For example, if a sentence has a negation like “No tiene fiebre”, BIO tagging shows that “No” is the beginning (**B-SCOPE**) and “tiene fiebre” is inside the scope (**I-SCOPE**), while other words would be labeled **O** if they are not part of it.\n",
    "\n",
    "Using BIO makes it easier for the model to learn patterns and detect complete scopes correctly, not just single words. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn_crfsuite import CRF\n",
    "from sklearn_crfsuite import metrics\n",
    "\n",
    "def to_bio_labels(labels, label_type=\"SCOPE\"):\n",
    "    # Convert lists of binary labels (0/1) into BIO tagging format for scopes\n",
    "    bio_labels = []\n",
    "    prefix = label_type.upper() + '_SCOPE'  # e.g., NEG_SCOPE or UNC_SCOPE\n",
    "    for sent in labels:\n",
    "        bio = []\n",
    "        prev = 0\n",
    "        for i, tag in enumerate(sent):\n",
    "            if tag == 1:\n",
    "                if i == 0 or prev == 0:\n",
    "                    bio.append(f'B-{prefix}')\n",
    "                else:\n",
    "                    bio.append(f'I-{prefix}')\n",
    "            else:\n",
    "                bio.append('O')\n",
    "            prev = tag\n",
    "        bio_labels.append(bio)\n",
    "    return bio_labels\n",
    "\n",
    "def df_to_crf_format(df):\n",
    "    # Convert a DataFrame into a list of feature dictionaries per sentence for CRF input\n",
    "    sentences = []\n",
    "    grouped = df.groupby(\"sentence_id\")\n",
    "    for _, group in grouped:\n",
    "        sentence = []\n",
    "        for _, row in group.iterrows():\n",
    "            features = {\n",
    "                'word.lower()': row['word'].lower(),\n",
    "                'word.isupper()': row['word'].isupper(),\n",
    "                'word.istitle()': row['word'].istitle(),\n",
    "                'pos': row['pos'],\n",
    "                'prefix': row['prefix'],\n",
    "                'suffix': row['suffix'],\n",
    "                'is_punct': row['is_punct'],\n",
    "                'in_single_word_cues': row['in_single_word_cues'],\n",
    "                'in_affixal_cues': row['in_affixal_cues'],\n",
    "                'ends_with_ment': row['ends_with_ment']\n",
    "            }\n",
    "            sentence.append(features)\n",
    "        sentences.append(sentence)\n",
    "    return sentences\n",
    "\n",
    "def df_to_labels(df, label_col):\n",
    "    # Extracts label sequences from the DataFrame, grouped by sentence\n",
    "    label_sequences = []\n",
    "    grouped = df.groupby(\"sentence_id\")\n",
    "    for _, group in grouped:\n",
    "        label_list = group[label_col].tolist()\n",
    "        label_sequences.append(label_list)\n",
    "    return label_sequences\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train + evaluate CRF model\n",
    "def train_and_evaluate_crf(df_train, df_test, label_col):\n",
    "    # Trains and evaluates a CRF model for BIO tagging using specified label column (e.g., 'neg_scope_label')\n",
    "    scope_type = \"NEG\" if \"neg\" in label_col.lower() else \"UNC\"\n",
    "\n",
    "    X_train = df_to_crf_format(df_train)\n",
    "    y_train_raw = df_to_labels(df_train, label_col)\n",
    "    y_train = to_bio_labels(y_train_raw, label_type=scope_type)\n",
    "\n",
    "    X_test = df_to_crf_format(df_test)\n",
    "    y_test_raw = df_to_labels(df_test, label_col)\n",
    "    y_test = to_bio_labels(y_test_raw, label_type=scope_type)\n",
    "\n",
    "    # Define class weights dynamically\n",
    "    if scope_type == \"NEG\":\n",
    "        class_weight = None  # Or {'B-NEG_SCOPE': 1.0, 'I-NEG_SCOPE': 1.0, 'O': 1.0}\n",
    "    else:  # UNC\n",
    "        class_weight = {\n",
    "            'B-UNC_SCOPE': 5.0,\n",
    "            'I-UNC_SCOPE': 5.0,\n",
    "            'O': 1.0\n",
    "        }\n",
    "\n",
    "    crf = CRF(algorithm='lbfgs', max_iterations=100, all_possible_transitions=True, \n",
    "              class_weight=class_weight)\n",
    "    crf.fit(X_train, y_train)\n",
    "    y_pred = crf.predict(X_test)\n",
    "\n",
    "    print(f\"CRF Evaluation for: {label_col.upper()}\")\n",
    "    print(metrics.flat_classification_report(y_test, y_pred))   \n",
    "    \n",
    "    return X_test, y_test, y_pred  # Return these variables for further use\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "We should try implementing something like a print to see how well it does in sentences (real examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_crf_predictions(df, X, y_true, y_pred, sentence_idx=0):\n",
    "    \"\"\"\n",
    "    Print words, true BIO labels, and predicted BIO labels for a given sentence index.\n",
    "    \"\"\"\n",
    "    grouped = df.groupby(\"sentence_id\")\n",
    "    sentence_ids = list(grouped.groups.keys())\n",
    "\n",
    "    if sentence_idx >= len(sentence_ids):\n",
    "        print(f\"Invalid sentence index {sentence_idx}. Max allowed: {len(sentence_ids) - 1}\")\n",
    "        return\n",
    "\n",
    "    sentence_id = sentence_ids[sentence_idx]\n",
    "    sentence_df = grouped.get_group(sentence_id)\n",
    "\n",
    "    print(f\"\\n--- Sentence {sentence_idx} (ID {sentence_id}) ---\")\n",
    "    print(f\"{'WORD':<15} {'TRUE':<15} {'PRED':<15}\")\n",
    "    print(f\"{'-'*45}\")\n",
    "    for i, row in sentence_df.iterrows():\n",
    "        word = row['word']\n",
    "        true_label = y_true[sentence_idx][row['token_id']]\n",
    "        pred_label = y_pred[sentence_idx][row['token_id']]\n",
    "        print(f\"{word:<15} {true_label:<15} {pred_label:<15}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CRF for negation scope detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "CRF.__init__() got an unexpected keyword argument 'class_weight'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[47], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# CRF BIO tagging evaluation for NEGATION scopes\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m X_test, y_test, y_pred \u001b[38;5;241m=\u001b[39m train_and_evaluate_crf(df_crf_neg_train, df_crf_neg_test, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mneg_scope_label\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m5\u001b[39m):\n\u001b[0;32m      4\u001b[0m     print_crf_predictions(df_crf_neg_test, X_test, y_test, y_pred, sentence_idx\u001b[38;5;241m=\u001b[39mi)\n",
      "Cell \u001b[1;32mIn[45], line 24\u001b[0m, in \u001b[0;36mtrain_and_evaluate_crf\u001b[1;34m(df_train, df_test, label_col)\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:  \u001b[38;5;66;03m# UNC\u001b[39;00m\n\u001b[0;32m     18\u001b[0m     class_weight \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m     19\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mB-UNC_SCOPE\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m5.0\u001b[39m,\n\u001b[0;32m     20\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mI-UNC_SCOPE\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m5.0\u001b[39m,\n\u001b[0;32m     21\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mO\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m1.0\u001b[39m\n\u001b[0;32m     22\u001b[0m     }\n\u001b[1;32m---> 24\u001b[0m crf \u001b[38;5;241m=\u001b[39m CRF(algorithm\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlbfgs\u001b[39m\u001b[38;5;124m'\u001b[39m, max_iterations\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m, all_possible_transitions\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, \n\u001b[0;32m     25\u001b[0m           class_weight\u001b[38;5;241m=\u001b[39mclass_weight)\n\u001b[0;32m     26\u001b[0m crf\u001b[38;5;241m.\u001b[39mfit(X_train, y_train)\n\u001b[0;32m     27\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m crf\u001b[38;5;241m.\u001b[39mpredict(X_test)\n",
      "\u001b[1;31mTypeError\u001b[0m: CRF.__init__() got an unexpected keyword argument 'class_weight'"
     ]
    }
   ],
   "source": [
    "\n",
    "# CRF BIO tagging evaluation for NEGATION scopes\n",
    "X_test, y_test, y_pred = train_and_evaluate_crf(df_crf_neg_train, df_crf_neg_test, \"neg_scope_label\")\n",
    "for i in range(5):\n",
    "    print_crf_predictions(df_crf_neg_test, X_test, y_test, y_pred, sentence_idx=i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CRF for uncertainty scope detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CRF Evaluation for: UNC_SCOPE_LABEL\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      " B-UNC_SCOPE       0.31      0.04      0.07       129\n",
      " I-UNC_SCOPE       0.17      0.06      0.09       437\n",
      "           O       0.99      1.00      0.99     64965\n",
      "\n",
      "    accuracy                           0.99     65531\n",
      "   macro avg       0.49      0.37      0.38     65531\n",
      "weighted avg       0.99      0.99      0.99     65531\n",
      "\n",
      "\n",
      "--- Sentence 100 (ID 100) ---\n",
      "WORD            TRUE            PRED           \n",
      "---------------------------------------------\n",
      "cardiovascular  O               O              \n",
      ":               O               O              \n",
      "auscultacion    O               O              \n",
      "cardiaca        O               O              \n",
      "con             O               O              \n",
      "tonos           O               O              \n",
      "ritmicos        O               O              \n",
      "y               O               O              \n",
      "sin             O               O              \n",
      "soplos          O               O              \n",
      ";               O               O              \n",
      "no              O               O              \n",
      "edemas          O               O              \n",
      "en              O               O              \n",
      "miembros        O               O              \n",
      "inferiores      O               O              \n",
      "ni              O               O              \n",
      "lesiones        O               O              \n",
      "cutaneas        O               O              \n",
      ".               O               O              \n",
      "\n",
      "--- Sentence 101 (ID 101) ---\n",
      "WORD            TRUE            PRED           \n",
      "---------------------------------------------\n",
      "respiratorio    O               O              \n",
      ":               O               O              \n",
      "auscultacion    O               O              \n",
      "respiratoria    O               O              \n",
      "con             O               O              \n",
      "murmullo        O               O              \n",
      "vesicular       O               O              \n",
      "conservado      O               O              \n",
      ",               O               O              \n",
      "sin             O               O              \n",
      "ruidos          O               O              \n",
      "patologicos     O               O              \n",
      "añadidos        O               O              \n",
      ".               O               O              \n",
      "\n",
      "--- Sentence 102 (ID 102) ---\n",
      "WORD            TRUE            PRED           \n",
      "---------------------------------------------\n",
      "abdomen         O               O              \n",
      ":               O               O              \n",
      "no              O               O              \n",
      "distendido      O               O              \n",
      ",               O               O              \n",
      "blando          O               O              \n",
      "y               O               O              \n",
      "depresible      O               O              \n",
      ",               O               O              \n",
      "no              O               O              \n",
      "doloroso        O               O              \n",
      "a               O               O              \n",
      "la              O               O              \n",
      "palpacion       O               O              \n",
      ".               O               O              \n",
      "\n",
      "--- Sentence 103 (ID 103) ---\n",
      "WORD            TRUE            PRED           \n",
      "---------------------------------------------\n",
      "no              O               O              \n",
      "se              O               O              \n",
      "palpan          O               O              \n",
      "masas           O               O              \n",
      "ni              O               O              \n",
      "megalias        O               O              \n",
      ",               O               O              \n",
      "ni              O               O              \n",
      "tampoco         O               O              \n",
      "globo           O               O              \n",
      "vesical         O               O              \n",
      ".               O               O              \n",
      "\n",
      "--- Sentence 104 (ID 104) ---\n",
      "WORD            TRUE            PRED           \n",
      "---------------------------------------------\n",
      "no              O               O              \n",
      "signos          O               O              \n",
      "de              O               O              \n",
      "irritacion      O               O              \n",
      "peritoneal      O               O              \n",
      ".               O               O              \n",
      "\n",
      "--- Sentence 105 (ID 105) ---\n",
      "WORD            TRUE            PRED           \n",
      "---------------------------------------------\n",
      "puñopercusion   O               O              \n",
      "lumbar          O               O              \n",
      "bilateral       O               O              \n",
      "negativa        O               O              \n",
      ".               O               O              \n"
     ]
    }
   ],
   "source": [
    "# CRF BIO tagging evaluation for UNCERTAINTY scopes\n",
    "X_test, y_test, y_pred = train_and_evaluate_crf(df_crf_unc_train, df_crf_unc_test, \"unc_scope_label\")\n",
    "for i in range(100,106):\n",
    "    print_crf_predictions(df_crf_unc_test, X_test, y_test, y_pred, sentence_idx=i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train UNC labels count: 0    237880\n",
      "1     14090\n",
      "Name: neg_scope_label, dtype: int64\n",
      "Test UNC labels count: 0    61938\n",
      "1     3593\n",
      "Name: neg_scope_label, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(\"Train UNC labels count:\", df_crf_neg_train[\"neg_scope_label\"].value_counts())\n",
    "print(\"Test UNC labels count:\", df_crf_neg_test[\"neg_scope_label\"].value_counts())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
